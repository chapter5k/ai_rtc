# ai_rtc/runner.py

from __future__ import annotations

import os
import math
import pickle
import csv

from datetime import datetime
from typing import Dict

import numpy as np
from numpy.typing import NDArray
from sklearn.utils import check_random_state
import torch
import torch.optim as optim
from tqdm import tqdm

from config import build_arg_parser, config_from_args, MainConfig
from utils import set_seed
from data_gen import ScenarioConfig, gen_reference_data
from cl_calib import estimate_CL_for_window, WindowCalib
from policy_nets import build_policy, save_policy, load_policy
from rl_pg import RLConfig, train_rl_policy
from eval_arl import evaluate_arl1
from benchmark import run_backend_benchmark  # ë„¤ê°€ ë§Œë“  ì´ë¦„ì— ë§ê²Œ ìˆ˜ì •


def _prepare_phase1_data(cfg: MainConfig, scen: ScenarioConfig, rng) -> NDArray:
    """S0_ref (Phase I ê¸°ì¤€ ë°ì´í„°)ë¥¼ ë¶ˆëŸ¬ì˜¤ê±°ë‚˜ ìƒˆë¡œ ìƒì„±."""
    if cfg.S0_ref_path and os.path.exists(cfg.S0_ref_path):
        print(f"[Phase I] ê¸°ì¡´ S0_ref ë¡œë“œ: {cfg.S0_ref_path}")
        S0_ref = np.load(cfg.S0_ref_path)
    else:
        print("[Phase I] S0_ref ìƒˆë¡œ ìƒì„± ì¤‘...")
        S0_ref = gen_reference_data(scen, rng)
        if cfg.S0_ref_path:
            os.makedirs(os.path.dirname(cfg.S0_ref_path), exist_ok=True)
            np.save(cfg.S0_ref_path, S0_ref)
            print(f"[Phase I] S0_ref ì €ì¥: {cfg.S0_ref_path}")
    return S0_ref


def _prepare_cl_calib(
    cfg: MainConfig,
    scen: ScenarioConfig,
    S0_ref: NDArray,
) -> Dict[int, WindowCalib]:
    """ìœˆë„ìš°ë³„ CL ë³´ì • ìˆ˜í–‰ í˜¹ì€ ê¸°ì¡´ calib_map ë¡œë“œ."""
    action_set = cfg.action_set

    if cfg.calib_map_path and os.path.exists(cfg.calib_map_path) and cfg.n_boot == 0:
        print(f"[CL] n_boot=0 & ê¸°ì¡´ calib_map ì‚¬ìš©: {cfg.calib_map_path}")
        with open(cfg.calib_map_path, "rb") as f:
            calib_map = pickle.load(f)
        return calib_map

    print(f"[CL] ë¶€íŠ¸ìŠ¤íŠ¸ë©ìœ¼ë¡œ CL ì¶”ì • ì‹œì‘ (n_boot={cfg.n_boot})")
    calib_map: Dict[int, WindowCalib] = {}
    for w in tqdm(action_set, desc="[CL] windowë³„ CL ì¶”ì •"):
        calib = estimate_CL_for_window(
            S0_ref,
            d=scen.d,
            window=w,
            n_boot=cfg.n_boot,
            n_estimators=cfg.n_estimators_eval,
            seed=cfg.seed,
            backend=cfg.rf_backend,
        )
        calib_map[w] = calib

    if cfg.calib_map_path:
        os.makedirs(os.path.dirname(cfg.calib_map_path), exist_ok=True)
        with open(cfg.calib_map_path, "wb") as f:
            pickle.dump(calib_map, f)
        print(f"[CL] calib_map ì €ì¥: {cfg.calib_map_path}")

    return calib_map


def _train_policy(
    cfg: MainConfig,
    scen: ScenarioConfig,
    calib_map: Dict[int, WindowCalib],
    S0_ref: NDArray,
) -> torch.nn.Module:
    """ì •ì±… ë„¤íŠ¸ì›Œí¬ ìƒì„± + RL í•™ìŠµ."""
    device = cfg.device
    action_set = cfg.action_set

    # 1) ì •ì±… ë„¤íŠ¸ì›Œí¬ ìƒì„±
    policy = build_policy(cfg.policy_arch, d=scen.d, num_actions=len(action_set))
    policy.to(device)

    # 2) ê¸°ì¡´ ê°€ì¤‘ì¹˜ ë¡œë“œ (ì„ íƒ)
    if cfg.policy_in and os.path.exists(cfg.policy_in):
        print(f"[RL] ê¸°ì¡´ ì •ì±… ë¡œë“œ: {cfg.policy_in}")
        policy = load_policy(
            path=cfg.policy_in,
            d=scen.d,
            num_actions=len(action_set),
            device=device,
            arch=cfg.policy_arch,
        )

    # 3) RL í•™ìŠµ
    rl_cfg = RLConfig(
        action_set=action_set,
        episodes=cfg.episodes,
        device=device,
    )
    optimizer = optim.Adam(policy.parameters(), lr=1e-3)

    print(f"[RL] Policy Gradient í•™ìŠµ ì‹œì‘ (episodes={cfg.episodes})")
    policy = train_rl_policy(
        policy=policy,
        optimizer=optimizer,
        cfg=rl_cfg,
        scen=scen,
        calib_map=calib_map,
        S0_ref=S0_ref,
        seed=cfg.seed,
        rf_backend=cfg.rf_backend,
        n_estimators_eval=cfg.n_estimators_eval,
    )

    # 4) í•™ìŠµëœ ì •ì±… ì €ì¥ (ì„ íƒ)
    if cfg.policy_out:
        os.makedirs(os.path.dirname(cfg.policy_out), exist_ok=True)
        save_policy(policy, cfg.policy_out)
        print(f"[RL] í•™ìŠµëœ ì •ì±… ì €ì¥: {cfg.policy_out}")

    return policy


def _evaluate(
    cfg: MainConfig,
    scen: ScenarioConfig,
    calib_map: Dict[int, WindowCalib],
    S0_ref: NDArray,
    policy: torch.nn.Module,
):
    """ì‹œë‚˜ë¦¬ì˜¤ I/II ì— ëŒ€í•´ ARL1 í‰ê°€ + CSV ì €ì¥."""
    lam2_list = [0.25, 0.50, 1.00, 2.00, 3.00, 5.00, 7.00, 9.00]
    lam_list = [math.sqrt(x) for x in lam2_list]

    # ì‹¤í–‰ í´ë” ì˜ˆ: outputs/run_20251113_153012/
    base_dir = os.path.dirname(cfg.S0_ref_path)  # ì´ë¯¸ runnerì—ì„œ ì„¸íŒ…í•¨

    for scenario_name in ["I", "II"]:
        print(f"\n[í‰ê°€] Scenario {scenario_name} (action_set={cfg.action_set})")

        arl_means, arl_stds = evaluate_arl1(
            scen_cfg=scen,
            lam_list=lam_list,
            scenario=scenario_name,
            policy=policy,
            actions=list(cfg.action_set),
            calib_map=calib_map,
            S0_ref=S0_ref,
            R=cfg.R,
            seed=cfg.seed,
            rf_backend=cfg.rf_backend,
            n_estimators_eval=cfg.n_estimators_eval,
        )

        # ---- CSV ì €ì¥ ----
        csv_path = os.path.join(base_dir, f"arl_results_scenario_{scenario_name}.csv")
        print(f"[ì €ì¥] {csv_path}")

        with open(csv_path, "w", encoding="utf-8", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["lambda2", "lambda", "arl1_mean", "arl1_std"])
            for lam2, lam, mean, std in zip(lam2_list, lam_list, arl_means, arl_stds):
                writer.writerow([lam2, lam, mean, std])

        # ---- ì½˜ì†” ì¶œë ¥ ----
        for lam2, lam, mean, std in zip(lam2_list, lam_list, arl_means, arl_stds):
            print(f"  Î»Â²={lam2:.2f} Î»={lam:.4f} ARL1={mean:.2f} [{std:.2f}]")


def main():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰: Phase I â†’ CL ë³´ì • â†’ RL í•™ìŠµ â†’ ARL1 í‰ê°€."""
    start_time = datetime.now()

    # 1) ì¸ì íŒŒì‹±
    parser = build_arg_parser()
    args = parser.parse_args()
    cfg = config_from_args(args)

    # ğŸ”½ğŸ”½ğŸ”½ ì—¬ê¸°ë¶€í„° ì¶”ê°€: ì¶œë ¥ í´ë” ì„¸íŒ… ğŸ”½ğŸ”½ğŸ”½
    # 1) ì‹¤í—˜ ì´ë¦„ ê²°ì •
    if cfg.exp_name is None or cfg.exp_name == "":
        exp_name = datetime.now().strftime("run_%Y%m%d_%H%M%S")
    else:
        exp_name = cfg.exp_name

    # 2) base_dir = outputs/run_...
    base_dir = os.path.join(cfg.outputs_dir, exp_name)
    os.makedirs(base_dir, exist_ok=True)
    print(f"[ì¶œë ¥] ê²°ê³¼ê°€ ì €ì¥ë  í´ë”: {base_dir}")

    # 3) S0_ref / calib_map / policy_out ì´ ë¹„ì–´ ìˆìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ ìë™ ì„¤ì •
    if not cfg.S0_ref_path:
        cfg.S0_ref_path = os.path.join(base_dir, "S0_ref.npy")
    if not cfg.calib_map_path:
        cfg.calib_map_path = os.path.join(base_dir, "calib_map.pkl")
    if not cfg.policy_out:
        cfg.policy_out = os.path.join(base_dir, "policy.pt")

    # (ì›í•˜ë©´ ARL ê²°ê³¼, ì„¤ì •ê°’ ì €ì¥ìš© ê¸°ë³¸ ê²½ë¡œë„ ë¯¸ë¦¬ ì¡ì•„ë‘˜ ìˆ˜ ìˆìŒ)
    # cfg.arl_results_path = os.path.join(base_dir, "arl_results.csv")
    # cfg.config_dump_path = os.path.join(base_dir, "config.txt")


    print(f"[ì‹œì‘] seed={cfg.seed}, device={cfg.device}, backend={cfg.rf_backend}")
    set_seed(cfg.seed)

    rng = check_random_state(cfg.seed)

    # 2) ì‹œë‚˜ë¦¬ì˜¤/ë°ì´í„° ì„¤ì • (ë…¼ë¬¸ ê¸°ë³¸ê°’)
    scen = ScenarioConfig(d=10, N0=1500, T=300, shift_time=100, sigma=1.0)

    # (ì„ íƒ) ë°±ì—”ë“œ ë²¤ì¹˜ë§ˆí¬
    try:
        from .benchmark import run_backend_benchmark
        run_backend_benchmark(
            d=scen.d,
            N0=scen.N0,
            backend=cfg.rf_backend,
            guess_arl1=cfg.guess_arl1,
            n_estimators=cfg.n_estimators_eval,
        )
    except Exception as e:
        print(f"[ë²¤ì¹˜ë§ˆí¬] ì‹¤íŒ¨ (ë¬´ì‹œí•´ë„ ë¨): {e}")

    # 3) Phase I ë°ì´í„° ì¤€ë¹„
    S0_ref = _prepare_phase1_data(cfg, scen, rng)

    # 4) CL ë³´ì •
    calib_map = _prepare_cl_calib(cfg, scen, S0_ref)

    # 5) RL ì •ì±… í•™ìŠµ
    policy = _train_policy(cfg, scen, calib_map, S0_ref)

    # 6) ARL1 í‰ê°€
    _evaluate(cfg, scen, calib_map, S0_ref, policy)

    elapsed = datetime.now() - start_time
    print(f"\n[ì™„ë£Œ] ì „ì²´ ì†Œìš” ì‹œê°„: {elapsed}")


if __name__ == "__main__":
    main()
