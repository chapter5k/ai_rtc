# Project Snapshot

- Generated at: `2025-11-13 12:28:49`
- Root directory: `C:\Users\USER\project\ai_rtc`

## Directory Tree

```text
./
  __init__.py
  benchmark.py
  cl_calib.py
  config.py
  data_gen.py
  eval_arl.py
  export_project_md.py
  policy_nets.py
  project_snapshot.md
  requirements.txt
  rl_pg.py
  rtc_backend.py
  runner.py
  utils.py
  ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰.docx

scripts/
  run_ai_rtc.py
```

## Python Files

### `__init__.py`

```python

```

### `benchmark.py`

```python

# ai_rtc/benchmark.py
# benchmark.py
import os
import time
from typing import Literal  # Literal ì•ˆ ì“°ë©´ ìƒëµí•´ë„ ë¨

import numpy as np
from numpy.typing import NDArray
from sklearn.utils import check_random_state

from rtc_backend import compute_pS0_stat


def run_backend_benchmark(
    S0_ref: NDArray,
    d: int,
    n_estimators: int,
    seed: int,
    backend: str,
) -> float:
    """ë‹¨ì¼ ë¶„ë¥˜ê¸° í˜¸ì¶œ ì‹œê°„ì„ ë²¤ì¹˜ë§ˆí‚¹ (ì´ˆ)"""
    rng_bench = check_random_state(seed)
    w_bench = 10
    start_idx = rng_bench.randint(0, len(S0_ref) - w_bench)
    Sw = S0_ref[start_idx: start_idx + w_bench]

    # S0_ref (ì •ìƒ) vs Sw (ë¹„ì •ìƒ) êµ¬ë¶„ í•™ìŠµ
    X = np.vstack([S0_ref, Sw])
    y = np.hstack([
        np.zeros(len(S0_ref), dtype=int),
        np.ones(len(Sw), dtype=int),
    ])

    t0 = time.perf_counter()

    if backend in ["sklearn", "cuml_cv"]:
        # RF ê²½ë¡œ: ê¸°ì¡´ ë¡œì§ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        compute_pS0_stat(
            X, y, np.arange(len(S0_ref)),
            d=d, n_estimators=n_estimators,
            seed=rng_bench.randint(1_000_000),
            backend=backend
        )

    elif backend == "lgbm":
        # âœ… LGBM ê²½ë¡œ: fit/predict ëª¨ë‘ ë„˜íŒŒì´ë¡œ í†µì¼ + GPU/CPU ìë™ ì„ íƒ
        from lightgbm import LGBMClassifier
        # 1) feature names ê²½ê³  ë°©ì§€: í•­ìƒ ndarray ì‚¬ìš©
        X_np = np.asarray(X, dtype=np.float32)
        y_np = np.asarray(y, dtype=np.int32)
        X_np = np.ascontiguousarray(X_np)
        y_np = np.ascontiguousarray(y_np)
        # 2) ì¥ì¹˜ ì„ íƒ (main()ì—ì„œ os.environ["AI_RTC_DEVICE"]=args.device ì„¤ì • í•„ìš”)
        use_gpu = os.environ.get("AI_RTC_DEVICE", "cpu").lower() == "cuda"
        model = LGBMClassifier(
            objective="binary",
            n_estimators=n_estimators,
            num_leaves=31,
            learning_rate=0.05,
            subsample=0.9,
            colsample_bytree=0.9,
            reg_lambda=1.0,
            verbose=-1,
            n_jobs=-1,
            device_type=("gpu" if use_gpu else "cpu"),
        )
        model.fit(X_np, y_np)
        _ = model.predict_proba(X_np)  # ì˜ˆì¸¡ê¹Œì§€ í¬í•¨í•´ ETAê°€ ì‹¤ì œì— ê°€ê¹ê²Œ

    else:
        raise ValueError(f"Unsupported backend: {backend}")

    return time.perf_counter() - t0

```

### `cl_calib.py`

```python
# cl_calib.py

from dataclasses import dataclass
import numpy as np
from numpy.typing import NDArray
from sklearn.utils import check_random_state
from tqdm import tqdm

from rtc_backend import compute_pS0_stat
from utils import _np2d, _np1d

@dataclass
class WindowCalib:
    """ìœˆë„ìš°ë³„ ê´€ë¦¬í•œê³„(CL)ì™€ ë¶€íŠ¸ìŠ¤íŠ¸ë© í‘œì¤€í¸ì°¨ë¥¼ ë‹´ëŠ” êµ¬ì¡°ì²´."""
    CL: float   # ê´€ë¦¬í•œê³„ (ìƒí•œì„ )
    std: float  # ë¶€íŠ¸ìŠ¤íŠ¸ë© std

def estimate_CL_for_window(
    S0: NDArray,
    d: int,
    window: int,
    n_boot: int,
    n_estimators: int,
    seed: int,
    backend: str = 'sklearn',
) -> WindowCalib:
    """
    ì£¼ì–´ì§„ window í¬ê¸°ì— ëŒ€í•´, ë¶€íŠ¸ìŠ¤íŠ¸ë©ìœ¼ë¡œ CL(ìƒí•œ)ì„ ì¶”ì •.
    ë°˜í™˜ê°’: WindowCalib(CL, std)
    """
    if n_boot <= 0:
        raise ValueError("estimate_CL_for_window: n_boot must be >= 1 (CL ìŠ¤í‚µì€ mainì—ì„œ ë¡œë“œ ë¶„ê¸°ë¥¼ ì‚¬ìš©).")
    
    rng = check_random_state(seed)
    alpha = 1.0 / 200.0   # ARL0 â‰ˆ 200 ì„ ë§ì¶”ê¸° ìœ„í•œ ìƒí•œ ë¶„ìœ„ìˆ˜
    stats = []
    N0 = len(S0)

    pbar = tqdm(range(n_boot), desc=f"  CL Boot (w={window})", leave=False, dynamic_ncols=True)
    for _ in pbar:
        start = 0 if N0 - window <= 0 else rng.randint(0, N0 - window)
        Sw = S0[start:start + window]

        # ì •ìƒ(S0) vs ë¹„ì •ìƒ(Sw)
        X = np.vstack([S0, Sw])
        y = np.hstack([
            np.zeros(len(S0), dtype=int),
            np.ones(len(Sw), dtype=int),
        ])

        # í•­ìƒ ë„˜íŒŒì´ë¡œ í†µì¼ (DataFrame â†’ ndarray í˜¼ìš© ë°©ì§€)
        X = _np2d(X, dtype=np.float32)
        y = _np1d(y, dtype=np.int32)

        pS0 = compute_pS0_stat(
            X, y, np.arange(len(S0)),
            d=d, n_estimators=n_estimators,
            seed=rng.randint(1_000_000),
            backend=backend,
        )
        stats.append(pS0)

    stats = np.asarray(stats)
    CL = np.quantile(stats, 1 - alpha)
    std_boot = float(np.std(stats, ddof=1))

    return WindowCalib(CL=CL, std=std_boot)

```

### `config.py`

```python
# ai_rtc/config.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Tuple, Optional, Literal

import argparse
import torch


RfBackend = Literal["sklearn", "cuml_cv", "lgbm"]
PolicyArch = Literal["cnn", "cnn_lstm"]

@dataclass
class MainConfig:
    """ë©”ì¸ ì‹¤í—˜ì—ì„œ ì“°ëŠ” í•µì‹¬ ì„¤ì •ê°’ ëª¨ìŒ."""
    seed: int = 2025
    device: str = "cuda"
    episodes: int = 30
    n_boot: int = 800
    action_set: Tuple[int, ...] = (5, 10, 15)
    R: int = 100
    rf_backend: RfBackend = "sklearn"
    guess_arl1: int = 15
    n_estimators_eval: int = 150
    policy_in: Optional[str] = None
    policy_out: Optional[str] = None
    S0_ref_path: str = ""
    calib_map_path: str = ""
    policy_arch: PolicyArch = "cnn_lstm"   # 2ë‹¨ê³„ê¹Œì§€ í–ˆìœ¼ë‹ˆ ê¸°ë³¸ê°’ cnn_lstm ë¡œ ì¡ì•„ë„ ë¨
    outputs_dir: str = "outputs"          # ê²°ê³¼ ë£¨íŠ¸ í´ë”
    exp_name: Optional[str] = None        # ì‹¤í—˜ ì´ë¦„ (ì—†ìœ¼ë©´ timestampë¡œ ìë™ ìƒì„±)
    
def build_arg_parser() -> argparse.ArgumentParser:
    """CLI ì¸ì ì •ì˜ (ì›ë˜ ai_rtc_251103_v4.pyì— ìˆë˜ argparse ë¶€ë¶„)."""
    parser = argparse.ArgumentParser(
        description="RL-RTC ë…¼ë¬¸ ì¬í˜„ + AI_RTC ì‹¤í—˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸"
    )

    default_device = "cuda" if torch.cuda.is_available() else "cpu"

    parser.add_argument('--seed', type=int, default=MainConfig.seed)
    parser.add_argument(
        '--device',
        type=str,
        default=default_device,
        help="PyTorch CNN ì—°ì‚° ì¥ì¹˜ ('cuda' ë˜ëŠ” 'cpu')"
    )
    parser.add_argument(
        '--episodes',
        type=int,
        default=MainConfig.episodes,
        help='RL í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜ (ê¶Œì¥ 100~300 ì´ìƒ)'
    )
    parser.add_argument(
        '--n_boot',
        type=int,
        default=MainConfig.n_boot,
        help='CL ë¶€íŠ¸ìŠ¤íŠ¸ë© ë°˜ë³µ (0ì´ë©´ CL ë³´ì • ìŠ¤í‚µ + ê¸°ì¡´ íŒŒì¼ ì‚¬ìš©)'
    )
    parser.add_argument(
        '--action_set',
        type=str,
        default="5,10,15",
        help='ìœˆë„ìš° í¬ê¸° ì˜µì…˜ (ì˜ˆ: "5,10,15" ë˜ëŠ” "3,10,17")'
    )
    parser.add_argument(
        '--R',
        type=int,
        default=MainConfig.R,
        help='ARL1 í‰ê°€ ë°˜ë³µ íšŸìˆ˜'
    )
    parser.add_argument(
        '--rf_backend',
        type=str,
        default=MainConfig.rf_backend,
        choices=['sklearn', 'cuml_cv', 'lgbm'],
        help="ë¶„ë¥˜ê¸° ë°±ì—”ë“œ: 'sklearn'(CPU,OOB), 'cuml_cv'(GPU,K-fold OOB), 'lgbm'(CPU/GPU)"
    )
    parser.add_argument(
        '--guess_arl1',
        type=int,
        default=MainConfig.guess_arl1,
        help='ARL1 í‰ê·  ì¶”ì •ì¹˜(ì •ì  ETA ê³„ì‚°ìš©, ì‚¬ìš© ì•ˆ í•´ë„ ë¬´ê´€)'
    )
    parser.add_argument(
        '--n_estimators_eval',
        type=int,
        default=MainConfig.n_estimators_eval,
        help='ARL1 í‰ê°€ ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•  íŠ¸ë¦¬ ìˆ˜'
    )
    parser.add_argument(
        '--policy_in',
        type=str,
        default=None,
        help='ë¶ˆëŸ¬ì˜¬ ì •ì±… ê°€ì¤‘ì¹˜(.pt). ì§€ì • ì‹œ ê±°ê¸°ì„œë¶€í„° í•™ìŠµ ê³„ì†'
    )
    parser.add_argument(
        '--policy_out',
        type=str,
        default=None,
        help='í•™ìŠµ í›„ ì €ì¥í•  ì •ì±… ê²½ë¡œ(.pt)'
    )
    parser.add_argument(
        '--S0_ref_path',
        type=str,
        default="",
        help="ê¸°ì¡´ Phase I ê¸°ì¤€ ë°ì´í„°(S0_ref .npy) ê²½ë¡œ (ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±)"
    )
    parser.add_argument(
        '--calib_map_path',
        type=str,
        default="",
        help="ê¸°ì¡´ CL ë³´ì • ë§µ .pkl ê²½ë¡œ (ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±)"
    )
    parser.add_argument(
        '--policy_arch',
        type=str,
        default=MainConfig.policy_arch,
        choices=['cnn', 'cnn_lstm'],
        help="ì •ì±… ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ì„ íƒ"
    )
    parser.add_argument(
        '--outputs_dir',
        type=str,
        default=MainConfig.outputs_dir,
        help="ê²°ê³¼ë¬¼ì„ ì €ì¥í•  ë£¨íŠ¸ í´ë” (ê¸°ë³¸: outputs)"
    )
    parser.add_argument(
        '--exp_name',
        type=str,
        default=None,
        help="ì‹¤í—˜ ì´ë¦„ (ë¯¸ì§€ì • ì‹œ run_YYYYMMDD_HHMMSS í˜•ì‹ìœ¼ë¡œ ìë™ ìƒì„±)"
    )

    return parser

def config_from_args(args: argparse.Namespace) -> MainConfig:
    """argparse ê²°ê³¼ë¥¼ dataclass(MainConfig)ë¡œ ë³€í™˜."""
    if isinstance(args.action_set, str):
        action_tuple = tuple(int(x) for x in args.action_set.split(','))
    else:
        action_tuple = tuple(args.action_set)

    return MainConfig(
        seed=args.seed,
        device=args.device,
        episodes=args.episodes,
        n_boot=args.n_boot,
        action_set=action_tuple,
        R=args.R,
        rf_backend=args.rf_backend,
        guess_arl1=args.guess_arl1,
        n_estimators_eval=args.n_estimators_eval,
        policy_in=args.policy_in,
        policy_out=args.policy_out,
        S0_ref_path=args.S0_ref_path,
        calib_map_path=args.calib_map_path,
        policy_arch=args.policy_arch,
        outputs_dir=args.outputs_dir,
        exp_name=args.exp_name,        
    )
```

### `data_gen.py`

```python
# ai_rtc/data_gen.py
from dataclasses import dataclass
from typing import Tuple
import math
import numpy as np
from numpy.typing import NDArray

@dataclass
class ScenarioConfig:
    d: int = 10             # ë³€ìˆ˜(ì„¼ì„œ) ì°¨ì›
    N0: int = 1500          # ì°¸ì¡°ë°ì´í„°(Phase I) í¬ê¸°
    T: int = 300            # Phase II ê¸¸ì´ (í•œ ì—í”¼ì†Œë“œ ê¸¸ì´)
    shift_time: int = 100   # t >= shift_time ë¶€í„° OOC (ARL1 í‰ê°€ëŠ” 0ìœ¼ë¡œ ì¬ì„¤ì •)
    sigma: float = 1.0      # ê³µë¶„ì‚°ì€ I (ë‹¨ìœ„ë¶„ì‚°) ê°€ì •


def make_cov(d: int) -> NDArray:
    return np.eye(d)


def gen_reference_data(cfg: ScenarioConfig, rng: np.random.RandomState) -> NDArray:
    return rng.multivariate_normal(mean=np.zeros(cfg.d), cov=make_cov(cfg.d), size=cfg.N0)


def make_phase2_series(cfg: ScenarioConfig, rng: np.random.RandomState, scenario: str, lam: float) -> Tuple[NDArray, NDArray]:
    """Returns X (T x d), labels_ic (T,) where labels_ic[t]=1 if in-control else 0."""
    X = rng.multivariate_normal(np.zeros(cfg.d), make_cov(cfg.d), size=cfg.T)
    labels_ic = np.ones(cfg.T, dtype=np.int64)

    if scenario.upper() == 'I':
        # Scenario I: exactly ONE variable is shifted by lam
        delta = np.zeros(cfg.d, dtype=float)
        delta[0] = lam
    else:
        # Scenario II: k = round(lam^2) variables are shifted
        k = max(1, int(round(lam**2)))
        k = min(k, cfg.d)
        a = lam / math.sqrt(k)     # keep total shift magnitude consistent
        delta = np.zeros(cfg.d, dtype=float)
        delta[:k] = a

    # apply mean shift at t >= shift_time
    if cfg.shift_time < cfg.T:
        X[cfg.shift_time:] += delta
        labels_ic[cfg.shift_time:] = 0

    return X.astype(np.float32), labels_ic

```

### `eval_arl.py`

```python
# ai_rtc/eval_arl.py
# eval_arl.py
from dataclasses import replace as _replace
from typing import List, Dict, Tuple

import numpy as np
from numpy.typing import NDArray
from sklearn.utils import check_random_state
from tqdm import tqdm

import torch  # policy.forward ì—ì„œ í•„ìš”í•˜ë©´

from data_gen import ScenarioConfig, make_phase2_series
from policy_nets import PolicyCNN          # íƒ€ì… íŒíŠ¸ìš©
from cl_calib import WindowCalib
from rtc_backend import compute_pS0_stat
from utils import _np2d, _np1d
from rl_pg import make_state_tensor        # ìƒíƒœ í…ì„œ ë§Œë“œëŠ” í•¨ìˆ˜


def run_length_until_alarm(
    X: NDArray,
    S0_ref: NDArray,
    policy: PolicyCNN,
    actions: List[int],
    calib_map: Dict[int, WindowCalib],
    d: int,
    rf_backend: str = 'sklearn',
    n_estimators_eval: int = 150,  # <-- ë§¤ê°œë³€ìˆ˜ ì¶”ê°€ (ê¸°ë³¸ê°’ 150)
) -> int:
    policy.eval()
    device = next(policy.parameters()).device
    T = len(X)

    # [DBG] ì •ì±… ì„ íƒ íˆìŠ¤í† ê·¸ë¨
    from collections import Counter
    action_chosen = Counter()    
    
    for t in range(1, T + 1):
        Lmax = 15
        w_for_state = min(max(actions), t)
        state = make_state_tensor(X[t - w_for_state:t], d, L=Lmax).to(device)
        with torch.no_grad():
            logits = policy(state)
            a_idx = int(torch.argmax(logits, dim=-1).item())

        w = actions[a_idx]
        
        # [DBG] ì„ íƒëœ ì°½ í¬ê¸° ì¹´ìš´íŠ¸
        action_chosen[w] += 1        
        
        # ì•„ì§ ì°½ì´ ë‹¤ ì•ˆ ì°¼ìœ¼ë©´ ì•ŒëŒ íŒë‹¨ì„ ë¯¸ë£¹ë‹ˆë‹¤.
        if t < w:
            continue
        Sw = X[t - w:t]
        Xrf = np.vstack([S0_ref, Sw])
        yrf = np.hstack([np.zeros(len(S0_ref), dtype=int), np.ones(len(Sw), dtype=int)])
        # âœ… í•­ìƒ ndarrayë¡œ í†µì¼(DFâ†’ndarray í˜¼ìš© ê²½ê³  ë°©ì§€)
        Xrf = _np2d(Xrf, dtype=np.float32)
        yrf = _np1d(yrf, dtype=np.int32)
        # pS0 = compute_pS0_stat(
        #     Xrf, yrf, np.arange(len(S0_ref)),
        #     d=d, n_estimators=n_estimators_eval, seed=42, backend=rf_backend
        # )        
        # í•­ìƒ ndarrayë¡œ í†µì¼ (feature names ê²½ê³  ë°©ì§€)
        Xrf = np.asarray(Xrf, dtype=np.float32)
        if Xrf.ndim == 1:
            Xrf = Xrf.reshape(1, -1)
        yrf = np.asarray(yrf, dtype=np.int32).ravel()
        pS0 = compute_pS0_stat(
            Xrf, yrf, np.arange(len(S0_ref)),
            d=d, n_estimators=n_estimators_eval, seed=42, backend=rf_backend
        )
        
        calib = calib_map[w]

        # [DBG] ì²« ê³„ì‚° ê°€ëŠ¥í•œ ì‹œì  ì£¼ë³€ì—ì„œ pS0ì™€ CLì„ í•¨ê»˜ ì¶œë ¥
        if t in (w, w + 1, w + 2):
            try:
                print(f"[dbg] t={t:4d}, w={w:2d}, pS0={pS0:.4f}, CL={calib.CL:.4f}")
            except Exception:
                pass
        
        if pS0 > calib.CL:
            # [DBG] ì•ŒëŒ ì‹œ ìµœì¢… ì„ íƒ íˆìŠ¤í† ê·¸ë¨ ì¶œë ¥
            try:
                print(f"[dbg] action histogram (until alarm t={t}): {dict(action_chosen)}")
            except Exception:
                pass
            
            return t
    # [DBG] ì•ŒëŒì´ ì•ˆ ë‚¬ì„ ë•Œë„ íˆìŠ¤í† ê·¸ë¨ ì¶œë ¥
    try:
        print(f"[dbg] action histogram (no alarm, T={T}): {dict(action_chosen)}")
    except Exception:
        pass
    return T
        

def evaluate_arl1(
    scen_cfg: ScenarioConfig,
    lam_list: List[float],
    scenario: str,
    policy: PolicyCNN,
    actions: List[int],
    calib_map: Dict[int, WindowCalib],
    S0_ref: NDArray,
    R: int,
    seed: int,
    rf_backend: str = 'sklearn',
    n_estimators_eval: int = 150,  # <-- ë§¤ê°œë³€ìˆ˜ ì¶”ê°€
) -> Tuple[List[float], List[float]]:
    arl_means: List[float] = []
    arl_stds: List[float] = []
    rng = check_random_state(seed)
    # ARL1: ì‹œí”„íŠ¸ ì¦‰ì‹œ ë°œìƒì„ ê°€ì •í•˜ë¯€ë¡œ shift_time=0ìœ¼ë¡œ ë³µì‚¬
    scen_cfg_eval = _replace(scen_cfg, shift_time=0)
    for lam in lam_list:
        print(f"  Evaluating lam^2={lam**2:.2f} (lam={lam:.4f}) (R={R} reps).")
        RLs = []
        pbar_R = tqdm(range(R), desc="    Reps", leave=False, dynamic_ncols=True)
        for _ in pbar_R:
            X, _ = make_phase2_series(scen_cfg_eval, rng, scenario, lam)
            rl = run_length_until_alarm(
                X, S0_ref, policy, actions, calib_map, scen_cfg.d,
                rf_backend=rf_backend, n_estimators_eval=n_estimators_eval
            )
            RLs.append(rl)
        arl_means.append(float(np.mean(RLs)))
        arl_stds.append(float(np.std(RLs, ddof=1)))
    return arl_means, arl_stds


```

### `policy_nets.py`

```python
# ai_rtc/policy_nets.py
import os
from typing import Literal
import torch
import torch.nn as nn
from utils import _unwrap_model

# -------------------------- RL ì •ì±… ë„¤íŠ¸ì›Œí¬ --------------------------
class PolicyCNN(nn.Module):
    """ë…¼ë¬¸ Table 1 êµ¬ì„±: Conv(1->16,k=4x4)->Sigmoid -> Conv(16->8,k=4x4)->Sigmoid -> Flatten(288) -> FC(288)->Sigmoid -> FC(#actions)"""
    def __init__(self, d: int, num_actions: int):
        super().__init__()
        if d != 10:
            raise ValueError("ì´ CNN êµ¬ì¡°ëŠ” ë…¼ë¬¸ ê¸°ì¤€ d=10ì— ë§ì¶°ì ¸ ìˆìŠµë‹ˆë‹¤.")
        self.conv1 = nn.Conv2d(1, 16, kernel_size=(4, 4))
        self.act1 = nn.Sigmoid()
        self.conv2 = nn.Conv2d(16, 8, kernel_size=(4, 4))
        self.act2 = nn.Sigmoid()
        H, W = 9, 4  # (15x10)->(12x7)->(9x4)
        flatten = 8 * H * W  # 288
        self.fc1 = nn.Linear(flatten, 288)
        self.act3 = nn.Sigmoid()
        self.fc2 = nn.Linear(288, num_actions)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        z = self.act1(self.conv1(x))
        z = self.act2(self.conv2(z))
        z = torch.flatten(z, 1)
        z = self.act3(self.fc1(z))
        logits = self.fc2(z)
        return logits

class PolicyCNNLSTM(nn.Module):
    """
    ì…ë ¥ x: (B, 1, L, d)  # make_state_tensor ê²°ê³¼, ê¸°ë³¸ L=15, d=10 ê°€ì •
    Conv1d(ì‹œê°„ì¶•) -> LSTM(128) -> FC(#actions)
    """
    def __init__(self, d: int, num_actions: int):
        super().__init__()
        if d != 10:
            raise ValueError("ì´ ëª¨ë¸ì€ í˜„ì¬ d=10 ê°€ì •ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        conv_out = 64
        # (B,1,L,d) -> (B,L,d) -> (B,d,L) í›„ Conv1d
        self.conv1 = nn.Conv1d(in_channels=d, out_channels=conv_out, kernel_size=3, padding=1)
        self.act1  = nn.ReLU()
        self.lstm  = nn.LSTM(input_size=conv_out, hidden_size=128, num_layers=1,
                             batch_first=True, bidirectional=False)
        self.fc    = nn.Linear(128, num_actions)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x.squeeze(1)           # (B, L, d)
        x = x.permute(0, 2, 1)     # (B, d, L)
        z = self.act1(self.conv1(x))   # (B, 64, L)
        z = z.permute(0, 2, 1)     # (B, L, 64)  # LSTM ì…ë ¥
        out, (h, c) = self.lstm(z)
        h_last = h[-1]             # (B, 128)
        logits = self.fc(h_last)   # (B, #actions)
        return logits

PolicyArch = Literal["cnn", "cnn_lstm"]


def build_policy(arch: PolicyArch, d: int, num_actions: int) -> nn.Module:
    if arch == 'cnn':
        return PolicyCNN(d=d, num_actions=num_actions)
    elif arch == 'cnn_lstm':
        return PolicyCNNLSTM(d=d, num_actions=num_actions)
    else:
        raise ValueError(f"Unknown policy_arch: {arch}")


def save_policy(policy: nn.Module, path: str):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    base = _unwrap_model(policy)
    torch.save(base.state_dict(), path)

def load_policy(path: str, d: int, num_actions: int, device: str, arch: str = 'cnn') -> nn.Module:
    policy = build_policy(arch, d=d, num_actions=num_actions)
    state = torch.load(path, map_location=device)
    policy.load_state_dict(state)
    policy.to(device)
    policy.eval()
    return policy


```

### `rl_pg.py`

```python
# ai_rtc/rl_pg.py
import math
from dataclasses import dataclass
from typing import List, Dict, Tuple

import numpy as np
from numpy.typing import NDArray
from sklearn.utils import check_random_state
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.optim as optim

from utils import _np2d, _np1d
from data_gen import ScenarioConfig, make_phase2_series
from cl_calib import WindowCalib
from rtc_backend import compute_pS0_stat



@dataclass
class RLConfig:
    action_set: Tuple[int, int, int] = (5, 10, 15)
    lr: float = 1e-3
    gamma: float = 0.99
    episodes: int = 50
    device: str = 'cpu'
    

def make_state_tensor(windowed: NDArray, d: int, L: int = 15) -> torch.Tensor:
    w = windowed.shape[0]
    if w >= L:
        data = windowed[-L:]
    else:
        pad = np.zeros((L - w, d), dtype=windowed.dtype)
        data = np.vstack([pad, windowed])
    t = torch.from_numpy(data.astype(np.float32)).unsqueeze(0).unsqueeze(0)
    return t


@dataclass
class WindowCalib:
    CL: float
    std: float
    size: int
    
def reward_by_algorithm1(action_idx: int, distances: List[float], in_control: bool) -> float:
    # distances: í° ê²ƒì´ ICì—ì„œ ì„ í˜¸ë˜ë„ë¡ ì„¤ê³„ë¨
    order = np.argsort(distances)[::-1]
    rank = int(np.where(order == action_idx)[0][0])
    return ({0: 1.0, 1: -1.0, 2: -2.0}[rank] if in_control else {0: -2.0, 1: -1.0, 2: 1.0}[rank])



def every(n, i):  # i: 0-based index
    return (i + 1) % n == 0

def train_rl_policy(
    policy: nn.Module,
    optimizer: optim.Optimizer,
    cfg: RLConfig,
    scen: ScenarioConfig,
    calib_map: Dict[int, WindowCalib],
    S0_ref: NDArray,
    seed: int,
    rf_backend: str = 'sklearn',
    n_estimators_eval: int = 150,
) -> nn.Module:
    rng = check_random_state(seed)
    policy.train()
    device = cfg.device
    if device.startswith('cuda') and torch.cuda.device_count() > 1:
        print("    > [RL] nn.DataParallel í™œì„±í™” (ë©€í‹° GPU ì‚¬ìš©)")
        policy = nn.DataParallel(policy)
    policy.to(device)
    if hasattr(torch, 'compile'):
        try:
            print("    > [RL] torch.compile() ì ìš© (PyTorch 2.x)")
            policy = torch.compile(policy)
        except Exception:
            pass
    actions = list(cfg.action_set)

    pbar_ep = tqdm(range(cfg.episodes), desc="[RL] Training", dynamic_ncols=True)
    for ep in pbar_ep:
        lam_choices_I = [math.sqrt(x) for x in [0.25, 0.5, 1, 2, 3, 5, 7, 9]]
        lam_choices_II = [math.sqrt(x) for x in [2, 3, 5, 7, 9]]
        if rng.rand() < 0.5:
            scenario = 'I'; lam = float(rng.choice(lam_choices_I))
        else:
            scenario = 'II'; lam = float(rng.choice(lam_choices_II))
        X, labels_ic = make_phase2_series(scen, rng, scenario, lam)

        logps: List[torch.Tensor] = []
        rewards: List[float] = []
        a_trace: List[int] = []

        for t in range(1, scen.T + 1):
            # --- ìœ íš¨ í–‰ë™ ë§ˆìŠ¤í¬(t>=w) ---
            valid = [t >= w for w in actions]
            if not any(valid):
                # ì•„ì§ ì–´ë–¤ ì°½ë„ ê½‰ ì°¨ì§€ ëª»í•œ ì‹œì ì´ë©´ ìƒíƒœë§Œ ê°±ì‹ 
                Lmax = 15
                w_for_state = min(max(actions), t)
                state = make_state_tensor(X[t - w_for_state:t], scen.d, L=Lmax).to(device)
                with torch.no_grad():
                    _ = policy(state)
                continue

            # --- ìœ íš¨ í–‰ë™ë§Œ í†µê³„/ê±°ë¦¬ ê³„ì‚° ---
            ms_list: List[float] = []
            D_list: List[float] = []
            valid_indices: List[int] = []
            for a_idx_tmp, w in enumerate(actions):
                if not valid[a_idx_tmp]:
                    continue
                Sw = X[t - w:t]
                Xrf = np.vstack([S0_ref, Sw])
                yrf = np.hstack([np.zeros(len(S0_ref), dtype=int), np.ones(len(Sw), dtype=int)])
                Xrf = _np2d(Xrf, dtype=np.float32)
                yrf = _np1d(yrf, dtype=np.int32)
                pS0 = compute_pS0_stat(
                    Xrf, yrf, np.arange(len(S0_ref)),
                    d=scen.d, n_estimators=n_estimators_eval,
                    seed=rng.randint(1_000_000), backend=rf_backend
                )
                ms_list.append(pS0)
                calib = calib_map[w]
                D = (calib.CL - pS0) / max(1e-8, calib.std)
                D_list.append(float(D))
                valid_indices.append(a_idx_tmp)

            # --- ì •ì±… ë¶„í¬ (ìœ íš¨í•˜ì§€ ì•Šì€ í–‰ë™ ë§ˆìŠ¤í‚¹) ---
            Lmax = 15
            w_for_state = min(max(actions), t)
            state = make_state_tensor(X[t - w_for_state:t], scen.d, L=Lmax).to(device)
            logits = policy(state)
            logits_masked = logits.clone()
            for idx, ok in enumerate(valid):
                if not ok:
                    logits_masked[0, idx] = -1e9
            probs = torch.softmax(logits_masked, dim=-1)
            m = torch.distributions.Categorical(probs=probs)
            a_idx_tensor = m.sample()
            a_idx = int(a_idx_tensor.item())

            a_trace.append(actions[a_idx])
            logps.append(m.log_prob(a_idx_tensor))

            # --- ë³´ìƒ(ìœ íš¨ í–‰ë™ì˜ ë¡œì»¬ ì¸ë±ìŠ¤ ê¸°ì¤€) ---
            if a_idx not in valid_indices:
                rewards.append(-2.0)  # ë°©ì–´ì  íŒ¨ë„í‹°(ì›ì¹™ì ìœ¼ë¡œ ë°œìƒí•˜ì§€ ì•ŠìŒ)
            else:
                local_idx = valid_indices.index(a_idx)
                ic = bool(labels_ic[t-1] == 1)
                rewards.append(float(reward_by_algorithm1(local_idx, D_list, in_control=ic)))

        # --- Policy Gradient ì—…ë°ì´íŠ¸ ---
        pbar_ep.set_postfix_str("Updating policy.")
        G = 0.0
        returns = []
        for r in reversed(rewards):
            G = r + cfg.gamma * G
            returns.append(G)
        returns = list(reversed(returns))
        returns_t = torch.tensor(returns, dtype=torch.float32, device=device)
        if len(returns_t) > 1:
            returns_t = (returns_t - returns_t.mean()) / (returns_t.std() + 1e-8)
        loss = 0.0
        for logp, Gt in zip(logps, returns_t):
            loss = loss - logp * Gt
        optimizer.zero_grad(); loss.backward()
        torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)
        optimizer.step()

        # --- 10 ì—í”¼ì†Œë“œë§ˆë‹¤ ìš”ì•½ ë¡œê·¸ ---
        if every(10, ep):
            avg_r = float(np.mean(rewards)) if len(rewards) else 0.0
            if len(a_trace):
                vals, counts = np.unique(a_trace, return_counts=True)
                pi_hist = {int(v): f"{float(c)/float(len(a_trace)):.2f}" for v, c in zip(vals, counts)}
            else:
                pi_hist = {}
            print(f"\n[ep {ep+1}] avg_reward={avg_r:.3f}  pi(w)={pi_hist}")

    if isinstance(policy, nn.DataParallel):
        policy = policy.module
    return policy


```

### `rtc_backend.py`

```python
import os
from typing import Literal
import numpy as np
from numpy.typing import NDArray
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils import check_random_state

from utils import sqrt_int, _np2d, _np1d

# -------------------------- p(S0,t) ê³„ì‚° ----------------------------

BackendName = Literal["sklearn", "cuml_cv", "lgbm"]

def compute_pS0_stat(
    X: NDArray,
    y: NDArray,
    idx_S0: NDArray,
    d: int,
    n_estimators: int,
    seed: int,
    backend: BackendName = "sklearn",
    kfold: int = 5,
) -> float:
    if backend == 'sklearn':
        rf = RandomForestClassifier(
            n_estimators=n_estimators,
            max_features=sqrt_int(d),
            bootstrap=True,
            oob_score=True,
            n_jobs=-1,
            random_state=seed,
        )
        rf.fit(X, y)
        oob = getattr(rf, 'oob_decision_function_', None)
        if oob is None:
            # ìƒ˜í”Œì´ ë„ˆë¬´ ì ì–´ oobê°€ ì—†ìœ¼ë©´ ì•ˆì „ fallback
            X_S0_np = X[idx_S0]
            if X_S0_np.ndim == 1:
                X_S0_np = X_S0_np.reshape(1, -1)
            proba = rf.predict_proba(X_S0_np)
            return float(np.mean(proba[:, 0]))
        p0 = oob[idx_S0, 0]
        return float(np.nanmean(p0))

    elif backend == 'lgbm':
        # LightGBM OOF í™•ë¥  ê¸°ë°˜ p(S0,t) ì¶”ì •
        try:
            from sklearn.model_selection import StratifiedKFold
            from lightgbm import LGBMClassifier
        except Exception as e:
            raise RuntimeError("LightGBMì´ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ backend='lgbm'ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. pip install lightgbm") from e
        
        use_gpu = os.environ.get("AI_RTC_DEVICE", "cpu").lower() == "cuda"

        # âœ… 1) ë¬´ì¡°ê±´ ë„˜íŒŒì´ë¡œ ìºìŠ¤íŒ… (DataFrame â†’ ndarray)
        #    astype(...)ëŠ” DFë¥¼ ì—¬ì „íˆ DFë¡œ ìœ ì§€í•˜ë¯€ë¡œ ê²½ê³  ì›ì¸ì´ ë¨
        X_np = np.asarray(X, dtype=np.float32)
        y_np = np.asarray(y, dtype=np.int32)

        # (ì„ íƒ) ë©”ëª¨ë¦¬ ì—°ì†ì„± í™•ë³´ â€“ ì¼ë¶€ í™˜ê²½ì—ì„œ ì•½ê°„ì˜ ì´ë“
        X_np = np.ascontiguousarray(X_np)
        y_np = np.ascontiguousarray(y_np)

        # --- í•µì‹¬ ê°€ë“œ: ì†Œìˆ˜ í´ë˜ìŠ¤ ê°œìˆ˜ì— ë§ì¶° n_splitsë¥¼ ì•ˆì „í•˜ê²Œ ì¡°ì • ---
        class_counts = np.bincount(y_np, minlength=2)
        min_class = int(class_counts.min())

        # --- Case A: ì†Œìˆ˜ í´ë˜ìŠ¤<2 â†’ CV ë¶ˆê°€ â†’ ê³µì •í•œ ë°±ì—…(sklearn OOB) ---
        if min_class < 2:
            from sklearn.ensemble import RandomForestClassifier as skRF
            sk = skRF(
                n_estimators=n_estimators,
                max_features=max(1, int(np.sqrt(X_np.shape[1]))),
                bootstrap=True,
                oob_score=True,
                random_state=seed,
                n_jobs=-1,
            )
            sk.fit(X_np, y_np)
            oob = getattr(sk, "oob_decision_function_", None)
            if oob is not None:
                return float(np.nanmean(oob[idx_S0, 0]))
            X_S0_np = X_np[idx_S0]
            if X_S0_np.ndim == 1:
                X_S0_np = X_S0_np.reshape(1, -1)
            proba = sk.predict_proba(X_S0_np)
            return float(np.mean(proba[:, 0]))

        # --- Case B: CV ê°€ëŠ¥ â†’ StratifiedKFoldë¡œ ì•ˆì •í™” ---
        try:
            n_splits = max(2, min(kfold, min_class))  # ì™¸ë¶€ kfold ì„¤ì •ì´ ìˆìœ¼ë©´ ì‚¬ìš©
        except NameError:
            n_splits = max(3, min(5, min_class))      # ë°±ì—… ë””í´íŠ¸(3~5)

        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)

        # ë¶ˆê· í˜• ì²˜ë¦¬ ê°€ì¤‘ì¹˜: scale_pos_weight = (#neg / #pos)
        n_pos = int((y_np == 1).sum())
        n_neg = int((y_np == 0).sum())
        scale_pos_weight = float(n_neg) / float(max(1, n_pos))

        # âœ… 2) scikit API ì‚¬ìš© (fit/predict ëª¨ë‘ ndarrayë¡œ í†µì¼)
        lgbm_params = dict(
            objective="binary",
            boosting_type="gbdt",
            n_estimators=200,          # RFì™€ ë¶„ë¦¬ëœ í•©ë¦¬ì  preset
            num_leaves=31,
            learning_rate=0.05,
            subsample=0.9,
            colsample_bytree=0.9,
            reg_lambda=1.0,
            random_state=seed,
            n_jobs=-1,
            verbose=-1,
            scale_pos_weight=max(1.0, scale_pos_weight),
        )
        
        lgbm_params["device_type"] = "gpu" if use_gpu else "cpu"

        p0_vals = []
        for tr_idx, te_idx in skf.split(X_np, y_np):
            y_tr_np = y_np[tr_idx]
            if np.unique(y_tr_np).size < 2:
                continue  # í•™ìŠµì„¸íŠ¸ê°€ ë‹¨ì¼í´ë˜ìŠ¤ë©´ ê±´ë„ˆëœ€

            model = LGBMClassifier(**lgbm_params)
            model.fit(X_np[tr_idx], y_tr_np)  # âœ… fit: ndarray

            # í…ŒìŠ¤íŠ¸ í´ë“œ ì¤‘ S0 ìœ„ì¹˜ë§Œ í™•ë¥  ì·¨í•©
            te_S0_idx = te_idx[np.isin(te_idx, idx_S0)]
            if te_S0_idx.size == 0:
                continue

            X_te_np = X_np[te_S0_idx]
            if X_te_np.ndim == 1:
                X_te_np = X_te_np.reshape(1, -1)

            proba = model.predict_proba(X_te_np)  # âœ… predict: ndarray
            p0_vals.append(proba[:, 0])           # class-0 í™•ë¥ ì´ p(S0,t)

            # ë©”ëª¨ë¦¬ ì •ë¦¬(í° ì‹¤í—˜ì—ì„œ ìœ ìš©)
            try:
                del model, proba
            except Exception:
                pass

        if not p0_vals:
            # ê·¹ë‹¨ ì¼€ì´ìŠ¤: ëª¨ë“  í´ë“œ ìŠ¤í‚µ â†’ sklearn OOBë¡œ ë°±ì—…
            from sklearn.ensemble import RandomForestClassifier as skRF
            sk = skRF(
                n_estimators=n_estimators,
                max_features=max(1, int(np.sqrt(X_np.shape[1]))),
                bootstrap=True,
                oob_score=True,
                random_state=seed,
                n_jobs=-1,
            )
            sk.fit(X_np, y_np)
            oob = getattr(sk, "oob_decision_function_", None)
            if oob is not None:
                return float(np.nanmean(oob[idx_S0, 0]))
            X_S0_np = X_np[idx_S0]
            if X_S0_np.ndim == 1:
                X_S0_np = X_S0_np.reshape(1, -1)
            proba = sk.predict_proba(X_S0_np)
            return float(np.mean(proba[:, 0]))

        return float(np.mean(np.concatenate(p0_vals)))

    elif backend == 'cuml_cv':
        try:
            import cupy as cp
            from cuml.ensemble import RandomForestClassifier as cuRF
            from sklearn.model_selection import StratifiedKFold
            mempool = cp.get_default_memory_pool() # <-- (ê¸°ì¡´ ìˆ˜ì •ì•ˆì— ì´ë¯¸ ìˆì–´ì•¼ í•¨)
        except Exception as e:
            raise RuntimeError("cuMLì´ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ backend='cuml_cv'ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. condaë¡œ RAPIDSë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.") from e

        X_np = X.astype(np.float32)
        y_np = y.astype(np.int32)

        # --- í•µì‹¬ ê°€ë“œ: ì†Œìˆ˜ í´ë˜ìŠ¤ ê°œìˆ˜ì— ë§ì¶° n_splitsë¥¼ ì•ˆì „í•˜ê²Œ ì¡°ì • ---
        class_counts = np.bincount(y_np, minlength=2)
        min_class = int(class_counts.min())

        # --- Case A: ì†Œìˆ˜ í´ë˜ìŠ¤<2 â†’ CV ë¶ˆê°€ â†’ ê³µì •í•œ ë°±ì—…(sklearn OOB) ---
        if min_class < 2:
            from sklearn.ensemble import RandomForestClassifier as skRF
            sk = skRF(
                n_estimators=n_estimators,
                max_features=max(1, int(np.sqrt(X_np.shape[1]))),
                bootstrap=True,
                oob_score=True,
                random_state=seed,
                n_jobs=-1,
            )
            sk.fit(X_np, y_np)
            oob = getattr(sk, "oob_decision_function_", None)
            if oob is not None:
                p0 = oob[idx_S0, 0]
                return float(np.nanmean(p0))
            X_S0_np = X_np[idx_S0]
            if X_S0_np.ndim == 1:
                X_S0_np = X_S0_np.reshape(1, -1)
            proba = sk.predict_proba(X_S0_np)
            return float(np.mean(proba[:, 0]))

        # --- Case B: CV ê°€ëŠ¥ â†’ StratifiedKFoldë¡œ ì•ˆì •í™” ---
        n_splits = max(2, min(kfold, min_class))
        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)

        p0_vals = []
        for tr_idx, te_idx in skf.split(X_np, y_np):
            y_tr_np = y_np[tr_idx]
            if np.unique(y_tr_np).size < 2:
                continue  # í•™ìŠµì„¸íŠ¸ê°€ ë‹¨ì¼í´ë˜ìŠ¤ë©´ ê±´ë„ˆëœ€
            # í•™ìŠµ(cuML)
            X_tr = cp.asarray(X_np[tr_idx]); y_tr = cp.asarray(y_tr_np)
            model = cuRF(
                n_estimators=n_estimators,
                max_features=max(1, int(np.sqrt(X_np.shape[1]))),
                bootstrap=True,
                n_streams=1,  # ì¬í˜„ì„± ê°•í™”
                random_state=seed,
            )
            model.fit(X_tr, y_tr)

            # í…ŒìŠ¤íŠ¸ì—ì„œ S0 ìœ„ì¹˜ë§Œ í™•ë¥  ì·¨í•©
            te_S0_mask = np.isin(te_idx, idx_S0)
            te_S0_idx = te_idx[te_S0_mask]
            if te_S0_idx.size == 0:
                continue
            X_te_np = X_np[te_S0_idx]
            if X_te_np.ndim == 1:
                X_te_np = X_te_np.reshape(1, -1)
            X_te = cp.asarray(X_te_np)
            proba = model.predict_proba(X_te)
            p0_vals.append(cp.asnumpy(proba)[:, 0])

# --- [ìˆ˜ì •] VRAM ëˆ„ìˆ˜ ë°©ì§€ë¥¼ ìœ„í•œ ëª…ì‹œì  ë©”ëª¨ë¦¬ í•´ì œ (íŒ¨ì¹˜ C) ---
            try:
                del model, X_tr, y_tr, X_te, proba
            except Exception:
                pass
            try:
                mempool.free_all_blocks()
                cp.get_default_pinned_memory_pool().free_all_blocks()
            except Exception:
                pass
            # -----------------------------------------------------


        if not p0_vals:
            # ê·¹ë‹¨ ì¼€ì´ìŠ¤: ëª¨ë“  í´ë“œ ìŠ¤í‚µ â†’ sklearn OOBë¡œ ë°±ì—…
            from sklearn.ensemble import RandomForestClassifier as skRF
            sk = skRF(
                n_estimators=n_estimators,
                max_features=max(1, int(np.sqrt(X_np.shape[1]))),
                bootstrap=True,
                oob_score=True,
                random_state=seed,
                n_jobs=-1,
            )
            sk.fit(X_np, y_np)
            oob = getattr(sk, "oob_decision_function_", None)
            if oob is not None:
                return float(np.nanmean(oob[idx_S0, 0]))
            X_S0_np = X_np[idx_S0]
            if X_S0_np.ndim == 1:
                X_S0_np = X_S0_np.reshape(1, -1)
            proba = sk.predict_proba(X_S0_np)
            return float(np.mean(proba[:, 0]))

        return float(np.mean(np.concatenate(p0_vals)))

    else:
        raise ValueError(f"Unknown backend: {backend}")


```

### `runner.py`

```python
# ai_rtc/runner.py

from __future__ import annotations

import os
import math
import pickle
import csv

from datetime import datetime
from typing import Dict

import numpy as np
from numpy.typing import NDArray
from sklearn.utils import check_random_state
import torch
import torch.optim as optim
from tqdm import tqdm

from config import build_arg_parser, config_from_args, MainConfig
from utils import set_seed
from data_gen import ScenarioConfig, gen_reference_data
from cl_calib import estimate_CL_for_window, WindowCalib
from policy_nets import build_policy, save_policy, load_policy
from rl_pg import RLConfig, train_rl_policy
from eval_arl import evaluate_arl1
from benchmark import run_backend_benchmark  


def _prepare_phase1_data(cfg: MainConfig, scen: ScenarioConfig, rng) -> NDArray:
    """S0_ref (Phase I ê¸°ì¤€ ë°ì´í„°)ë¥¼ ë¶ˆëŸ¬ì˜¤ê±°ë‚˜ ìƒˆë¡œ ìƒì„±."""
    if cfg.S0_ref_path and os.path.exists(cfg.S0_ref_path):
        print(f"[Phase I] ê¸°ì¡´ S0_ref ë¡œë“œ: {cfg.S0_ref_path}")
        S0_ref = np.load(cfg.S0_ref_path)
    else:
        print("[Phase I] S0_ref ìƒˆë¡œ ìƒì„± ì¤‘...")
        S0_ref = gen_reference_data(scen, rng)
        if cfg.S0_ref_path:
            os.makedirs(os.path.dirname(cfg.S0_ref_path), exist_ok=True)
            np.save(cfg.S0_ref_path, S0_ref)
            print(f"[Phase I] S0_ref ì €ì¥: {cfg.S0_ref_path}")
    return S0_ref


def _prepare_cl_calib(
    cfg: MainConfig,
    scen: ScenarioConfig,
    S0_ref: NDArray,
) -> Dict[int, WindowCalib]:
    """ìœˆë„ìš°ë³„ CL ë³´ì • ìˆ˜í–‰ í˜¹ì€ ê¸°ì¡´ calib_map ë¡œë“œ."""
    action_set = cfg.action_set

    if cfg.calib_map_path and os.path.exists(cfg.calib_map_path) and cfg.n_boot == 0:
        print(f"[CL] n_boot=0 & ê¸°ì¡´ calib_map ì‚¬ìš©: {cfg.calib_map_path}")
        with open(cfg.calib_map_path, "rb") as f:
            calib_map = pickle.load(f)
        return calib_map

    print(f"[CL] ë¶€íŠ¸ìŠ¤íŠ¸ë©ìœ¼ë¡œ CL ì¶”ì • ì‹œì‘ (n_boot={cfg.n_boot})")
    calib_map: Dict[int, WindowCalib] = {}
    for w in tqdm(action_set, desc="[CL] windowë³„ CL ì¶”ì •"):
        calib = estimate_CL_for_window(
            S0_ref,
            d=scen.d,
            window=w,
            n_boot=cfg.n_boot,
            n_estimators=cfg.n_estimators_eval,
            seed=cfg.seed,
            backend=cfg.rf_backend,
        )
        calib_map[w] = calib

    if cfg.calib_map_path:
        os.makedirs(os.path.dirname(cfg.calib_map_path), exist_ok=True)
        with open(cfg.calib_map_path, "wb") as f:
            pickle.dump(calib_map, f)
        print(f"[CL] calib_map ì €ì¥: {cfg.calib_map_path}")

    return calib_map


def _train_policy(
    cfg: MainConfig,
    scen: ScenarioConfig,
    calib_map: Dict[int, WindowCalib],
    S0_ref: NDArray,
) -> torch.nn.Module:
    """ì •ì±… ë„¤íŠ¸ì›Œí¬ ìƒì„± + RL í•™ìŠµ."""
    device = cfg.device
    action_set = cfg.action_set

    # 1) ì •ì±… ë„¤íŠ¸ì›Œí¬ ìƒì„±
    policy = build_policy(cfg.policy_arch, d=scen.d, num_actions=len(action_set))
    policy.to(device)

    # 2) ê¸°ì¡´ ê°€ì¤‘ì¹˜ ë¡œë“œ (ì„ íƒ)
    if cfg.policy_in and os.path.exists(cfg.policy_in):
        print(f"[RL] ê¸°ì¡´ ì •ì±… ë¡œë“œ: {cfg.policy_in}")
        policy = load_policy(
            path=cfg.policy_in,
            d=scen.d,
            num_actions=len(action_set),
            device=device,
            arch=cfg.policy_arch,
        )

    # 3) RL í•™ìŠµ
    rl_cfg = RLConfig(
        action_set=action_set,
        episodes=cfg.episodes,
        device=device,
    )
    optimizer = optim.Adam(policy.parameters(), lr=1e-3)

    print(f"[RL] Policy Gradient í•™ìŠµ ì‹œì‘ (episodes={cfg.episodes})")
    policy = train_rl_policy(
        policy=policy,
        optimizer=optimizer,
        cfg=rl_cfg,
        scen=scen,
        calib_map=calib_map,
        S0_ref=S0_ref,
        seed=cfg.seed,
        rf_backend=cfg.rf_backend,
        n_estimators_eval=cfg.n_estimators_eval,
    )

    # 4) í•™ìŠµëœ ì •ì±… ì €ì¥ (ì„ íƒ)
    if cfg.policy_out:
        os.makedirs(os.path.dirname(cfg.policy_out), exist_ok=True)
        save_policy(policy, cfg.policy_out)
        print(f"[RL] í•™ìŠµëœ ì •ì±… ì €ì¥: {cfg.policy_out}")

    return policy


def _evaluate(
    cfg: MainConfig,
    scen: ScenarioConfig,
    calib_map: Dict[int, WindowCalib],
    S0_ref: NDArray,
    policy: torch.nn.Module,
):
    """ì‹œë‚˜ë¦¬ì˜¤ I/II ì— ëŒ€í•´ ARL1 í‰ê°€ + CSV ì €ì¥."""
    lam2_list = [0.25, 0.50, 1.00, 2.00, 3.00, 5.00, 7.00, 9.00]
    lam_list = [math.sqrt(x) for x in lam2_list]

    # ì‹¤í–‰ í´ë” ì˜ˆ: outputs/run_20251113_153012/
    base_dir = os.path.dirname(cfg.S0_ref_path)  # ì´ë¯¸ runnerì—ì„œ ì„¸íŒ…í•¨

    for scenario_name in ["I", "II"]:
        print(f"\n[í‰ê°€] Scenario {scenario_name} (action_set={cfg.action_set})")

        arl_means, arl_stds = evaluate_arl1(
            scen_cfg=scen,
            lam_list=lam_list,
            scenario=scenario_name,
            policy=policy,
            actions=list(cfg.action_set),
            calib_map=calib_map,
            S0_ref=S0_ref,
            R=cfg.R,
            seed=cfg.seed,
            rf_backend=cfg.rf_backend,
            n_estimators_eval=cfg.n_estimators_eval,
        )

        # ---- CSV ì €ì¥ ----
        csv_path = os.path.join(base_dir, f"arl_results_scenario_{scenario_name}.csv")
        print(f"[ì €ì¥] {csv_path}")

        with open(csv_path, "w", encoding="utf-8", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["lambda2", "lambda", "arl1_mean", "arl1_std"])
            for lam2, lam, mean, std in zip(lam2_list, lam_list, arl_means, arl_stds):
                writer.writerow([lam2, lam, mean, std])

        # ---- ì½˜ì†” ì¶œë ¥ ----
        for lam2, lam, mean, std in zip(lam2_list, lam_list, arl_means, arl_stds):
            print(f"  Î»Â²={lam2:.2f} Î»={lam:.4f} ARL1={mean:.2f} [{std:.2f}]")


def main():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰: Phase I â†’ CL ë³´ì • â†’ RL í•™ìŠµ â†’ ARL1 í‰ê°€."""
    start_time = datetime.now()

    # 1) ì¸ì íŒŒì‹±
    parser = build_arg_parser()
    args = parser.parse_args()
    cfg = config_from_args(args)

    # ğŸ”½ğŸ”½ğŸ”½ ì—¬ê¸°ë¶€í„° ì¶”ê°€: ì¶œë ¥ í´ë” ì„¸íŒ… ğŸ”½ğŸ”½ğŸ”½
    # 1) ì‹¤í—˜ ì´ë¦„ ê²°ì •
    if cfg.exp_name is None or cfg.exp_name == "":
        exp_name = datetime.now().strftime("run_%Y%m%d_%H%M%S")
    else:
        exp_name = cfg.exp_name

    # 2) base_dir = outputs/run_...
    base_dir = os.path.join(cfg.outputs_dir, exp_name)
    os.makedirs(base_dir, exist_ok=True)
    print(f"[ì¶œë ¥] ê²°ê³¼ê°€ ì €ì¥ë  í´ë”: {base_dir}")

    # 3) S0_ref / calib_map / policy_out ì´ ë¹„ì–´ ìˆìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ ìë™ ì„¤ì •
    if not cfg.S0_ref_path:
        cfg.S0_ref_path = os.path.join(base_dir, "S0_ref.npy")
    if not cfg.calib_map_path:
        cfg.calib_map_path = os.path.join(base_dir, "calib_map.pkl")
    if not cfg.policy_out:
        cfg.policy_out = os.path.join(base_dir, "policy.pt")

    # (ì›í•˜ë©´ ARL ê²°ê³¼, ì„¤ì •ê°’ ì €ì¥ìš© ê¸°ë³¸ ê²½ë¡œë„ ë¯¸ë¦¬ ì¡ì•„ë‘˜ ìˆ˜ ìˆìŒ)
    # cfg.arl_results_path = os.path.join(base_dir, "arl_results.csv")
    # cfg.config_dump_path = os.path.join(base_dir, "config.txt")


    print(f"[ì‹œì‘] seed={cfg.seed}, device={cfg.device}, backend={cfg.rf_backend}")
    set_seed(cfg.seed)

    rng = check_random_state(cfg.seed)

    # 2) ì‹œë‚˜ë¦¬ì˜¤/ë°ì´í„° ì„¤ì • (ë…¼ë¬¸ ê¸°ë³¸ê°’)
    scen = ScenarioConfig(d=10, N0=1500, T=300, shift_time=100, sigma=1.0)
    
    # 3) Phase I ë°ì´í„° ì¤€ë¹„
    S0_ref = _prepare_phase1_data(cfg, scen, rng)

    # âœ… (ì„ íƒ) ë°±ì—”ë“œ ë²¤ì¹˜ë§ˆí¬ - ì •ìƒ ë²„ì „
    try:
        elapsed = run_backend_benchmark(
            S0_ref=S0_ref,
            d=scen.d,
            n_estimators=cfg.n_estimators_eval,
            seed=cfg.seed,
            backend=cfg.rf_backend,
        )
        print(f"[ë²¤ì¹˜ë§ˆí¬] backend='{cfg.rf_backend}' ê¸°ì¤€ 1íšŒ í†µê³„ ê³„ì‚° ì‹œê°„ â‰ˆ {elapsed:.3f} ì´ˆ")
    except Exception as e:
        print(f"[ë²¤ì¹˜ë§ˆí¬] ì‹¤íŒ¨ (ë¬´ì‹œí•´ë„ ë¨): {e}")

    # 4) CL ë³´ì •
    calib_map = _prepare_cl_calib(cfg, scen, S0_ref)

    # 5) RL ì •ì±… í•™ìŠµ
    policy = _train_policy(cfg, scen, calib_map, S0_ref)

    # 6) ARL1 í‰ê°€
    _evaluate(cfg, scen, calib_map, S0_ref, policy)

    elapsed = datetime.now() - start_time
    print(f"\n[ì™„ë£Œ] ì „ì²´ ì†Œìš” ì‹œê°„: {elapsed}")


if __name__ == "__main__":
    main()

```

### `scripts\run_ai_rtc.py`

```python
# scripts/run_ai_rtc.py

import os
import sys

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(CURRENT_DIR, ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

from runner import main   

if __name__ == "__main__":
    main()

```

### `utils.py`

```python
# ai_rtc/utils.py
import os
import math
import random
import numpy as np
import torch
import torch.nn as nn


def _np1d(y, dtype=np.int32):
    a = np.asarray(y, dtype=dtype).ravel()
    return np.ascontiguousarray(a)

def _np2d(X, dtype=np.float32):
    A = np.asarray(X, dtype=dtype)
    if A.ndim == 1:
        A = A.reshape(1, -1)
    return np.ascontiguousarray(A)

def set_seed(seed: int = 1234):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

def sqrt_int(x: float) -> int:
    return max(1, int(round(math.sqrt(x))))

def _unwrap_model(m: nn.Module) -> nn.Module:
    # torch.compile ë˜í•‘ í•´ì œ
    if hasattr(m, "_orig_mod"):
        m = m._orig_mod
    # DataParallel ë˜í•‘ í•´ì œ
    if isinstance(m, nn.DataParallel):
        m = m.module
    return m

```
