# Project Snapshot

- Generated at: `2025-11-20 09:45:21`
- Root directory: `C:\Users\USER\project\ai_rtc`

## Directory Tree

```text
./
  __init__.py
  benchmark.py
  cl_calib.py
  config.py
  data_gen.py
  eval_arl.py
  export_project_md.py
  policy_nets.py
  project_snapshot.md
  requirements.txt
  reward_morl.py
  rl_pg.py
  rl_sac.py
  rtc_backend.py
  runner.py
  utils.py
  ~$í¬ë¦½íŠ¸ ì‹¤í–‰.docx
  ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰.docx

scripts/
  run_ai_rtc.py
```

## Python Files

### `__init__.py`

```python

```

### `benchmark.py`

```python

# ai_rtc/benchmark.py
# benchmark.py
import os
import time
from typing import Literal  # Literal ì•ˆ ì“°ë©´ ìƒëµí•´ë„ ë¨

import numpy as np
from numpy.typing import NDArray
from sklearn.utils import check_random_state

from rtc_backend import compute_pS0_stat


def run_backend_benchmark(
    S0_ref: NDArray,
    d: int,
    n_estimators: int,
    seed: int,
    backend: str,
) -> float:
    """ë‹¨ì¼ ë¶„ë¥˜ê¸° í˜¸ì¶œ ì‹œê°„ì„ ë²¤ì¹˜ë§ˆí‚¹ (ì´ˆ)"""
    rng_bench = check_random_state(seed)
    w_bench = 10
    start_idx = rng_bench.randint(0, len(S0_ref) - w_bench)
    Sw = S0_ref[start_idx: start_idx + w_bench]

    # S0_ref (ì •ìƒ) vs Sw (ë¹„ì •ìƒ) êµ¬ë¶„ í•™ìŠµ
    X = np.vstack([S0_ref, Sw])
    y = np.hstack([
        np.zeros(len(S0_ref), dtype=int),
        np.ones(len(Sw), dtype=int),
    ])

    t0 = time.perf_counter()

    if backend in ["sklearn", "cuml_cv"]:
        # RF ê²½ë¡œ: ê¸°ì¡´ ë¡œì§ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        compute_pS0_stat(
            X, y, np.arange(len(S0_ref)),
            d=d, n_estimators=n_estimators,
            seed=rng_bench.randint(1_000_000),
            backend=backend
        )

    elif backend == "lgbm":
        # âœ… LGBM ê²½ë¡œ: fit/predict ëª¨ë‘ ë„˜íŒŒì´ë¡œ í†µì¼ + GPU/CPU ìë™ ì„ íƒ
        from lightgbm import LGBMClassifier
        # 1) feature names ê²½ê³  ë°©ì§€: í•­ìƒ ndarray ì‚¬ìš©
        X_np = np.asarray(X, dtype=np.float32)
        y_np = np.asarray(y, dtype=np.int32)
        X_np = np.ascontiguousarray(X_np)
        y_np = np.ascontiguousarray(y_np)
        # 2) ì¥ì¹˜ ì„ íƒ (main()ì—ì„œ os.environ["AI_RTC_DEVICE"]=args.device ì„¤ì • í•„ìš”)
        use_gpu = os.environ.get("AI_RTC_DEVICE", "cpu").lower() == "cuda"
        model = LGBMClassifier(
            objective="binary",
            n_estimators=n_estimators,
            num_leaves=31,
            learning_rate=0.05,
            subsample=0.9,
            colsample_bytree=0.9,
            reg_lambda=1.0,
            verbose=-1,
            n_jobs=-1,
            device_type=("gpu" if use_gpu else "cpu"),
        )
        model.fit(X_np, y_np)
        _ = model.predict_proba(X_np)  # ì˜ˆì¸¡ê¹Œì§€ í¬í•¨í•´ ETAê°€ ì‹¤ì œì— ê°€ê¹ê²Œ

    else:
        raise ValueError(f"Unsupported backend: {backend}")

    return time.perf_counter() - t0

```

### `cl_calib.py`

```python
# cl_calib.py

from dataclasses import dataclass
import numpy as np
from numpy.typing import NDArray
from sklearn.utils import check_random_state
from tqdm import tqdm

from rtc_backend import compute_pS0_stat
from utils import _np2d, _np1d

@dataclass
class WindowCalib:
    """ìœˆë„ìš°ë³„ ê´€ë¦¬í•œê³„(CL)ì™€ ë¶€íŠ¸ìŠ¤íŠ¸ë© í‘œì¤€í¸ì°¨ë¥¼ ë‹´ëŠ” êµ¬ì¡°ì²´."""
    CL: float   # ê´€ë¦¬í•œê³„ (ìƒí•œì„ )
    std: float  # ë¶€íŠ¸ìŠ¤íŠ¸ë© std

def estimate_CL_for_window(
    S0: NDArray,
    d: int,
    window: int,
    n_boot: int,
    n_estimators: int,
    seed: int,
    target_arl0: float = 200.0,   # ğŸ‘ˆ ìƒˆ ì¸ì (ê¸°ë³¸ê°’ 200)
    backend: str = 'sklearn',
) -> WindowCalib:
    """
    ì£¼ì–´ì§„ window í¬ê¸°ì— ëŒ€í•´, ë¶€íŠ¸ìŠ¤íŠ¸ë©ìœ¼ë¡œ CL(ìƒí•œ)ì„ ì¶”ì •.
    ë°˜í™˜ê°’: WindowCalib(CL, std)
    """
    if n_boot <= 0:
        raise ValueError("estimate_CL_for_window: n_boot must be >= 1 (CL ìŠ¤í‚µì€ mainì—ì„œ ë¡œë“œ ë¶„ê¸°ë¥¼ ì‚¬ìš©).")
    
    rng = check_random_state(seed)
    alpha = 1.0 / float(target_arl0)   # ARL0 â‰ˆ 200 ì„ ë§ì¶”ê¸° ìœ„í•œ ìƒí•œ ë¶„ìœ„ìˆ˜
    stats = []
    N0 = len(S0)

    pbar = tqdm(range(n_boot), desc=f"  CL Boot (w={window})", leave=False, dynamic_ncols=True)
    for _ in pbar:
        start = 0 if N0 - window <= 0 else rng.randint(0, N0 - window)
        Sw = S0[start:start + window]

        # ì •ìƒ(S0) vs ë¹„ì •ìƒ(Sw)
        X = np.vstack([S0, Sw])
        y = np.hstack([
            np.zeros(len(S0), dtype=int),
            np.ones(len(Sw), dtype=int),
        ])

        # í•­ìƒ ë„˜íŒŒì´ë¡œ í†µì¼ (DataFrame â†’ ndarray í˜¼ìš© ë°©ì§€)
        X = _np2d(X, dtype=np.float32)
        y = _np1d(y, dtype=np.int32)

        pS0 = compute_pS0_stat(
            X, y, np.arange(len(S0)),
            d=d, n_estimators=n_estimators,
            seed=rng.randint(1_000_000),
            backend=backend,
        )
        stats.append(pS0)

    stats = np.asarray(stats)
    CL = np.quantile(stats, 1 - alpha)
    std_boot = float(np.std(stats, ddof=1))

    return WindowCalib(CL=CL, std=std_boot)

```

### `config.py`

```python
# ai_rtc/config.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Tuple, Optional, Literal

import argparse
import torch


RfBackend = Literal["sklearn", "cuml_cv", "lgbm"]
PolicyArch = Literal["cnn", "cnn_lstm"]
AlgoType = Literal["pg", "sac_discrete"]
RewardType = Literal["alg1", "morl"]   #

@dataclass
class MainConfig:
    """ë©”ì¸ ì‹¤í—˜ì—ì„œ ì“°ëŠ” í•µì‹¬ ì„¤ì •ê°’ ëª¨ìŒ."""
    seed: int = 2025
    device: str = "cuda"
    episodes: int = 30
    n_boot: int = 800
    action_set: Tuple[int, ...] = (5, 10, 15)
    R: int = 100
    rf_backend: RfBackend = "sklearn"
    guess_arl1: int = 15
    n_estimators_eval: int = 150
    policy_in: Optional[str] = None
    policy_out: Optional[str] = None
    S0_ref_path: str = ""
    calib_map_path: str = ""
    outputs_dir: str = "outputs"          # ê²°ê³¼ ë£¨íŠ¸ í´ë”
    exp_name: Optional[str] = None        # ì‹¤í—˜ ì´ë¦„ (ì—†ìœ¼ë©´ timestampë¡œ ìë™ ìƒì„±)
    policy_arch: Literal["cnn", "cnn_lstm"] = "cnn_lstm"
    algo: Literal["pg", "sac_discrete"] = "pg"   # ê¸°ë³¸ê°’ PG
    rl_lr: float = 1e-3
    reward: RewardType = "alg1"
    target_arl0: int = 200   # CL ë³´ì • ì‹œ ëª©í‘œ ARL0 (ê¸°ë³¸=200)
    
def build_arg_parser() -> argparse.ArgumentParser:
    """CLI ì¸ì ì •ì˜ (ì›ë˜ ai_rtc_251103_v4.pyì— ìˆë˜ argparse ë¶€ë¶„)."""
    parser = argparse.ArgumentParser(
        description="RL-RTC ë…¼ë¬¸ ì¬í˜„ + AI_RTC ì‹¤í—˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸"
    )

    default_device = "cuda" if torch.cuda.is_available() else "cpu"

    parser.add_argument('--seed', type=int, default=MainConfig.seed)
    parser.add_argument(
        '--device',
        type=str,
        default=default_device,
        help="PyTorch CNN ì—°ì‚° ì¥ì¹˜ ('cuda' ë˜ëŠ” 'cpu')"
    )
    parser.add_argument(
        '--episodes',
        type=int,
        default=MainConfig.episodes,
        help='RL í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜ (ê¶Œì¥ 100~300 ì´ìƒ)'
    )
    parser.add_argument(
        '--n_boot',
        type=int,
        default=MainConfig.n_boot,
        help='CL ë¶€íŠ¸ìŠ¤íŠ¸ë© ë°˜ë³µ (0ì´ë©´ CL ë³´ì • ìŠ¤í‚µ + ê¸°ì¡´ íŒŒì¼ ì‚¬ìš©)'
    )
    parser.add_argument(
        '--action_set',
        type=str,
        default="5,10,15",
        help='ìœˆë„ìš° í¬ê¸° ì˜µì…˜ (ì˜ˆ: "5,10,15" ë˜ëŠ” "3,10,17")'
    )
    parser.add_argument(
        '--R',
        type=int,
        default=MainConfig.R,
        help='ARL1 í‰ê°€ ë°˜ë³µ íšŸìˆ˜'
    )
    parser.add_argument(
        '--rf_backend',
        type=str,
        default=MainConfig.rf_backend,
        choices=['sklearn', 'cuml_cv', 'lgbm'],
        help="ë¶„ë¥˜ê¸° ë°±ì—”ë“œ: 'sklearn'(CPU,OOB), 'cuml_cv'(GPU,K-fold OOB), 'lgbm'(CPU/GPU)"
    )
    parser.add_argument(
        '--guess_arl1',
        type=int,
        default=MainConfig.guess_arl1,
        help='ARL1 í‰ê·  ì¶”ì •ì¹˜(ì •ì  ETA ê³„ì‚°ìš©, ì‚¬ìš© ì•ˆ í•´ë„ ë¬´ê´€)'
    )
    parser.add_argument(
        '--n_estimators_eval',
        type=int,
        default=MainConfig.n_estimators_eval,
        help='ARL1 í‰ê°€ ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•  íŠ¸ë¦¬ ìˆ˜'
    )
    parser.add_argument(
        '--policy_in',
        type=str,
        default=None,
        help='ë¶ˆëŸ¬ì˜¬ ì •ì±… ê°€ì¤‘ì¹˜(.pt). ì§€ì • ì‹œ ê±°ê¸°ì„œë¶€í„° í•™ìŠµ ê³„ì†'
    )
    parser.add_argument(
        '--policy_out',
        type=str,
        default=None,
        help='í•™ìŠµ í›„ ì €ì¥í•  ì •ì±… ê²½ë¡œ(.pt)'
    )
    parser.add_argument(
        '--S0_ref_path',
        type=str,
        default="",
        help="ê¸°ì¡´ Phase I ê¸°ì¤€ ë°ì´í„°(S0_ref .npy) ê²½ë¡œ (ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±)"
    )
    parser.add_argument(
        '--calib_map_path',
        type=str,
        default="",
        help="ê¸°ì¡´ CL ë³´ì • ë§µ .pkl ê²½ë¡œ (ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±)"
    )
    parser.add_argument(
        '--policy_arch',
        type=str,
        default=MainConfig.policy_arch,
        choices=['cnn', 'cnn_lstm'],
        help="ì •ì±… ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ì„ íƒ"
    )
    parser.add_argument(
        '--outputs_dir',
        type=str,
        default=MainConfig.outputs_dir,
        help="ê²°ê³¼ë¬¼ì„ ì €ì¥í•  ë£¨íŠ¸ í´ë” (ê¸°ë³¸: outputs)"
    )
    parser.add_argument(
        '--exp_name',
        type=str,
        default=None,
        help="ì‹¤í—˜ ì´ë¦„ (ë¯¸ì§€ì • ì‹œ run_YYYYMMDD_HHMMSS í˜•ì‹ìœ¼ë¡œ ìë™ ìƒì„±)"
    )
    parser.add_argument(
        '--algo',
        type=str,
        default=MainConfig.algo,
        choices=['pg', 'sac_discrete'],
        help="ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ì„ íƒ: 'pg'(ê¸°ì¡´ Policy Gradient) ë˜ëŠ” 'sac_discrete'(ì´ì‚° SAC)"
    )
    parser.add_argument(
        '--rl_lr',
        type=float,
        default=MainConfig.rl_lr,
        help="Policy Gradient(LR) í•™ìŠµë¥  (algo='pg'ì¼ ë•Œ ì‚¬ìš©)"
    )
    parser.add_argument(
        '--reward',
        type=str,
        default="alg1",
        choices=['alg1', 'morl'],
        help="ë³´ìƒ ì„¤ê³„: 'alg1'(ë…¼ë¬¸ Algorithm 1), 'morl'(ARL0/ARL1 íŠ¸ë ˆì´ë“œì˜¤í”„ìš© MORL ìŠ¤ì¹¼ë¼í™”)"
    )
    parser.add_argument(
        "--target_arl0",
        type=int,
        default=200,
        help="CL ë³´ì • ì‹œ ëª©í‘œ ARL0 (ê¸°ë³¸=200). alpha=1/ARL0 ë¡œ CL ë¶„ìœ„ìˆ˜ ê³„ì‚°"
    )

    return parser

def config_from_args(args: argparse.Namespace) -> MainConfig:
    """argparse ê²°ê³¼ë¥¼ dataclass(MainConfig)ë¡œ ë³€í™˜."""
    if isinstance(args.action_set, str):
        action_tuple = tuple(int(x) for x in args.action_set.split(','))
    else:
        action_tuple = tuple(args.action_set)

    return MainConfig(
        seed=args.seed,
        device=args.device,
        episodes=args.episodes,
        n_boot=args.n_boot,
        action_set=action_tuple,
        R=args.R,
        rf_backend=args.rf_backend,
        guess_arl1=args.guess_arl1,
        n_estimators_eval=args.n_estimators_eval,
        policy_in=args.policy_in,
        policy_out=args.policy_out,
        S0_ref_path=args.S0_ref_path,
        calib_map_path=args.calib_map_path,
        policy_arch=args.policy_arch,
        outputs_dir=args.outputs_dir,
        exp_name=args.exp_name,
        algo=args.algo,
        rl_lr=args.rl_lr,
        reward=args.reward,
        target_arl0=args.target_arl0,
    )
```

### `data_gen.py`

```python
# ai_rtc/data_gen.py
from dataclasses import dataclass
from typing import Tuple
import math
import numpy as np
from numpy.typing import NDArray

@dataclass
class ScenarioConfig:
    d: int = 10             # ë³€ìˆ˜(ì„¼ì„œ) ì°¨ì›
    N0: int = 1500          # ì°¸ì¡°ë°ì´í„°(Phase I) í¬ê¸°
    T: int = 300            # Phase II ê¸¸ì´ (í•œ ì—í”¼ì†Œë“œ ê¸¸ì´)
    shift_time: int = 100   # t >= shift_time ë¶€í„° OOC (ARL1 í‰ê°€ëŠ” 0ìœ¼ë¡œ ì¬ì„¤ì •)
    sigma: float = 1.0      # ê³µë¶„ì‚°ì€ I (ë‹¨ìœ„ë¶„ì‚°) ê°€ì •


def make_cov(d: int) -> NDArray:
    return np.eye(d)


def gen_reference_data(cfg: ScenarioConfig, rng: np.random.RandomState) -> NDArray:
    return rng.multivariate_normal(mean=np.zeros(cfg.d), cov=make_cov(cfg.d), size=cfg.N0)


def make_phase2_series(cfg: ScenarioConfig, rng: np.random.RandomState, scenario: str, lam: float) -> Tuple[NDArray, NDArray]:
    """Returns X (T x d), labels_ic (T,) where labels_ic[t]=1 if in-control else 0."""
    X = rng.multivariate_normal(np.zeros(cfg.d), make_cov(cfg.d), size=cfg.T)
    labels_ic = np.ones(cfg.T, dtype=np.int64)

    if scenario.upper() == 'I':
        # Scenario I: exactly ONE variable is shifted by lam
        delta = np.zeros(cfg.d, dtype=float)
        delta[0] = lam
    else:
        # Scenario II: k = round(lam^2) variables are shifted
        k = max(1, int(round(lam**2)))
        k = min(k, cfg.d)
        a = lam / math.sqrt(k)     # keep total shift magnitude consistent
        delta = np.zeros(cfg.d, dtype=float)
        delta[:k] = a

    # apply mean shift at t >= shift_time
    if cfg.shift_time < cfg.T:
        X[cfg.shift_time:] += delta
        labels_ic[cfg.shift_time:] = 0

    return X.astype(np.float32), labels_ic

```

### `eval_arl.py`

```python
# ai_rtc/eval_arl.py
# eval_arl.py
from dataclasses import replace as _replace
from typing import List, Dict, Tuple

import numpy as np
from numpy.typing import NDArray
from sklearn.utils import check_random_state
from tqdm import tqdm

import torch  # policy.forward ì—ì„œ í•„ìš”í•˜ë©´

from data_gen import ScenarioConfig, make_phase2_series
from policy_nets import PolicyCNN          # íƒ€ì… íŒíŠ¸ìš©
from cl_calib import WindowCalib
from rtc_backend import compute_pS0_stat
from utils import _np2d, _np1d
from rl_pg import make_state_tensor        # ìƒíƒœ í…ì„œ ë§Œë“œëŠ” í•¨ìˆ˜



def run_length_until_alarm(
    X: NDArray,
    S0_ref: NDArray,
    policy: PolicyCNN,
    actions: List[int],
    calib_map: Dict[int, WindowCalib],
    d: int,
    rf_backend: str = 'sklearn',
    n_estimators_eval: int = 150,  # <-- ë§¤ê°œë³€ìˆ˜ ì¶”ê°€ (ê¸°ë³¸ê°’ 150)
) -> int:
    policy.eval()
    device = next(policy.parameters()).device
    T = len(X)

    # [DBG] ì •ì±… ì„ íƒ íˆìŠ¤í† ê·¸ë¨
    from collections import Counter
    action_chosen = Counter()    
    
    for t in range(1, T + 1):
        Lmax = 15
        w_for_state = min(max(actions), t)
        state = make_state_tensor(X[t - w_for_state:t], d, L=Lmax).to(device)
        with torch.no_grad():
            logits = policy(state)
            a_idx = int(torch.argmax(logits, dim=-1).item())

        w = actions[a_idx]
        
        # [DBG] ì„ íƒëœ ì°½ í¬ê¸° ì¹´ìš´íŠ¸
        action_chosen[w] += 1        
        
        # ì•„ì§ ì°½ì´ ë‹¤ ì•ˆ ì°¼ìœ¼ë©´ ì•ŒëŒ íŒë‹¨ì„ ë¯¸ë£¹ë‹ˆë‹¤.
        if t < w:
            continue
        Sw = X[t - w:t]
        Xrf = np.vstack([S0_ref, Sw])
        yrf = np.hstack([np.zeros(len(S0_ref), dtype=int), np.ones(len(Sw), dtype=int)])
        # âœ… í•­ìƒ ndarrayë¡œ í†µì¼(DFâ†’ndarray í˜¼ìš© ê²½ê³  ë°©ì§€)
        Xrf = _np2d(Xrf, dtype=np.float32)
        yrf = _np1d(yrf, dtype=np.int32)
        # pS0 = compute_pS0_stat(
        #     Xrf, yrf, np.arange(len(S0_ref)),
        #     d=d, n_estimators=n_estimators_eval, seed=42, backend=rf_backend
        # )        
        # í•­ìƒ ndarrayë¡œ í†µì¼ (feature names ê²½ê³  ë°©ì§€)
        Xrf = np.asarray(Xrf, dtype=np.float32)
        if Xrf.ndim == 1:
            Xrf = Xrf.reshape(1, -1)
        yrf = np.asarray(yrf, dtype=np.int32).ravel()
        pS0 = compute_pS0_stat(
            Xrf, yrf, np.arange(len(S0_ref)),
            d=d, n_estimators=n_estimators_eval, seed=42, backend=rf_backend
        )
        
        calib = calib_map[w]

        # [DBG] ì²« ê³„ì‚° ê°€ëŠ¥í•œ ì‹œì  ì£¼ë³€ì—ì„œ pS0ì™€ CLì„ í•¨ê»˜ ì¶œë ¥
        if t in (w, w + 1, w + 2):
            try:
                print(f"[dbg] t={t:4d}, w={w:2d}, pS0={pS0:.4f}, CL={calib.CL:.4f}")
            except Exception:
                pass
        
        if pS0 > calib.CL:
            # [DBG] ì•ŒëŒ ì‹œ ìµœì¢… ì„ íƒ íˆìŠ¤í† ê·¸ë¨ ì¶œë ¥
            try:
                print(f"[dbg] action histogram (until alarm t={t}): {dict(action_chosen)}")
            except Exception:
                pass
            
            return t
    # [DBG] ì•ŒëŒì´ ì•ˆ ë‚¬ì„ ë•Œë„ íˆìŠ¤í† ê·¸ë¨ ì¶œë ¥
    try:
        print(f"[dbg] action histogram (no alarm, T={T}): {dict(action_chosen)}")
    except Exception:
        pass
    return T
        
def evaluate_arl0(
    scen_cfg: ScenarioConfig,
    scenario: str,
    policy: PolicyCNN,
    actions: List[int],
    calib_map: Dict[int, WindowCalib],
    S0_ref: NDArray,
    R: int,
    seed: int,
    rf_backend: str = 'sklearn',
    n_estimators_eval: int = 150,
) -> Tuple[float, float]:
    """
    ARL0(ì •ìƒ ìƒíƒœì—ì„œì˜ í‰ê·  Run Length)ë¥¼ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ì¶”ì •.

    - Phase II ì „ì²´ êµ¬ê°„ì´ in-control ì´ ë˜ë„ë¡ shift_timeì„ Të¡œ ë°€ì–´ë²„ë¦° ë’¤,
    - lam=0.0 ìœ¼ë¡œ make_phase2_series ë¥¼ ì—¬ëŸ¬ ë²ˆ ìƒì„±í•´ run_length_until_alarm ì˜ í‰ê· ì„ êµ¬í•¨.
    """
    rng = check_random_state(seed)
    RLs: List[int] = []

    # shift_time=T ë¡œ ì„¤ì •í•˜ì—¬ ë³€í™”ê°€ ì•„ì˜ˆ ë°œìƒí•˜ì§€ ì•Šë„ë¡ ë§Œë“ ë‹¤.
    scen_cfg_ic = _replace(scen_cfg, shift_time=scen_cfg.T)

    pbar = tqdm(range(R), desc="  ARL0 sim", leave=False, dynamic_ncols=True)
    for _ in pbar:
        # lam=0.0 ì´ë©´ scenario I/II ìƒê´€ì—†ì´ mean shift ì—†ìŒ
        X, labels_ic = make_phase2_series(scen_cfg_ic, rng, scenario, lam=0.0)

        rl = run_length_until_alarm(
            X=X,
            S0_ref=S0_ref,
            policy=policy,
            actions=actions,
            calib_map=calib_map,
            d=scen_cfg_ic.d,
            rf_backend=rf_backend,
            n_estimators_eval=n_estimators_eval,
        )
        RLs.append(rl)

    mean = float(np.mean(RLs))
    std = float(np.std(RLs, ddof=1))
    return mean, std

def evaluate_arl1(
    scen_cfg: ScenarioConfig,
    lam_list: List[float],
    scenario: str,
    policy: PolicyCNN,
    actions: List[int],
    calib_map: Dict[int, WindowCalib],
    S0_ref: NDArray,
    R: int,
    seed: int,
    rf_backend: str = 'sklearn',
    n_estimators_eval: int = 150,  # <-- ë§¤ê°œë³€ìˆ˜ ì¶”ê°€
) -> Tuple[List[float], List[float]]:
    arl_means: List[float] = []
    arl_stds: List[float] = []
    rng = check_random_state(seed)
    # ARL1: ì‹œí”„íŠ¸ ì¦‰ì‹œ ë°œìƒì„ ê°€ì •í•˜ë¯€ë¡œ shift_time=0ìœ¼ë¡œ ë³µì‚¬
    scen_cfg_eval = _replace(scen_cfg, shift_time=0)
    for lam in lam_list:
        print(f"  Evaluating lam^2={lam**2:.2f} (lam={lam:.4f}) (R={R} reps).")
        RLs = []
        pbar_R = tqdm(range(R), desc="    Reps", leave=False, dynamic_ncols=True)
        for _ in pbar_R:
            X, _ = make_phase2_series(scen_cfg_eval, rng, scenario, lam)
            rl = run_length_until_alarm(
                X, S0_ref, policy, actions, calib_map, scen_cfg.d,
                rf_backend=rf_backend, n_estimators_eval=n_estimators_eval
            )
            RLs.append(rl)
        arl_means.append(float(np.mean(RLs)))
        arl_stds.append(float(np.std(RLs, ddof=1)))
    return arl_means, arl_stds


```

### `policy_nets.py`

```python
# ai_rtc/policy_nets.py
import os
from typing import Literal
import torch
import torch.nn as nn
from utils import _unwrap_model

# -------------------------- RL ì •ì±… ë„¤íŠ¸ì›Œí¬ --------------------------
class PolicyCNN(nn.Module):
    """ë…¼ë¬¸ Table 1 êµ¬ì„±: Conv(1->16,k=4x4)->Sigmoid -> Conv(16->8,k=4x4)->Sigmoid -> Flatten(288) -> FC(288)->Sigmoid -> FC(#actions)"""
    def __init__(self, d: int, num_actions: int):
        super().__init__()
        if d != 10:
            raise ValueError("ì´ CNN êµ¬ì¡°ëŠ” ë…¼ë¬¸ ê¸°ì¤€ d=10ì— ë§ì¶°ì ¸ ìˆìŠµë‹ˆë‹¤.")
        self.conv1 = nn.Conv2d(1, 16, kernel_size=(4, 4))
        self.act1 = nn.Sigmoid()
        self.conv2 = nn.Conv2d(16, 8, kernel_size=(4, 4))
        self.act2 = nn.Sigmoid()
        H, W = 9, 4  # (15x10)->(12x7)->(9x4)
        flatten = 8 * H * W  # 288
        self.fc1 = nn.Linear(flatten, 288)
        self.act3 = nn.Sigmoid()
        self.fc2 = nn.Linear(288, num_actions)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        z = self.act1(self.conv1(x))
        z = self.act2(self.conv2(z))
        z = torch.flatten(z, 1)
        z = self.act3(self.fc1(z))
        logits = self.fc2(z)
        return logits

class PolicyCNNLSTM(nn.Module):
    """
    ì…ë ¥ x: (B, 1, L, d)  # make_state_tensor ê²°ê³¼, ê¸°ë³¸ L=15, d=10 ê°€ì •
    Conv1d(ì‹œê°„ì¶•) -> LSTM(128) -> FC(#actions)
    """
    def __init__(self, d: int, num_actions: int):
        super().__init__()
        if d != 10:
            raise ValueError("ì´ ëª¨ë¸ì€ í˜„ì¬ d=10 ê°€ì •ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        conv_out = 64
        # (B,1,L,d) -> (B,L,d) -> (B,d,L) í›„ Conv1d
        self.conv1 = nn.Conv1d(in_channels=d, out_channels=conv_out, kernel_size=3, padding=1)
        self.act1  = nn.ReLU()
        self.lstm  = nn.LSTM(input_size=conv_out, hidden_size=128, num_layers=1,
                             batch_first=True, bidirectional=False)
        self.fc    = nn.Linear(128, num_actions)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x.squeeze(1)           # (B, L, d)
        x = x.permute(0, 2, 1)     # (B, d, L)
        z = self.act1(self.conv1(x))   # (B, 64, L)
        z = z.permute(0, 2, 1)     # (B, L, 64)  # LSTM ì…ë ¥
        out, (h, c) = self.lstm(z)
        h_last = h[-1]             # (B, 128)
        logits = self.fc(h_last)   # (B, #actions)
        return logits

PolicyArch = Literal["cnn", "cnn_lstm"]


def build_policy(arch: PolicyArch, d: int, num_actions: int) -> nn.Module:
    if arch == 'cnn':
        return PolicyCNN(d=d, num_actions=num_actions)
    elif arch == 'cnn_lstm':
        return PolicyCNNLSTM(d=d, num_actions=num_actions)
    else:
        raise ValueError(f"Unknown policy_arch: {arch}")


def save_policy(policy: nn.Module, path: str):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    base = _unwrap_model(policy)
    torch.save(base.state_dict(), path)

def load_policy(path: str, d: int, num_actions: int, device: str, arch: str = 'cnn') -> nn.Module:
    policy = build_policy(arch, d=d, num_actions=num_actions)
    state = torch.load(path, map_location=device)
    policy.load_state_dict(state)
    policy.to(device)
    policy.eval()
    return policy


```

### `reward_morl.py`

```python
# ai_rtc/reward_morl.py
from __future__ import annotations

from dataclasses import dataclass, field
from typing import List, Dict, Tuple
import numpy as np

# ê¸°ì¡´ Algorithm 1 ë³´ìƒ(ìˆœìœ„ ê¸°ë°˜)ê³¼ ë™ì¼í•œ í˜•íƒœë¡œ ë¡œì»¬ ì •ì˜
def reward_by_algorithm1(action_idx: int, distances: List[float], in_control: bool) -> float:
    """
    distances: IC êµ¬ê°„ì—ì„œ 'í´ìˆ˜ë¡ ì¢‹ì€' ì ìˆ˜ (ì˜ˆ: (CL - pS0) / std)
    action_idx: ì „ì²´ actions ì¤‘ì—ì„œ ì—ì´ì „íŠ¸ê°€ ì„ íƒí•œ í–‰ë™ì˜ ì¸ë±ìŠ¤ (0, 1, 2)

    - IC(in_control=True)   : rankê°€ ë†’ì„ìˆ˜ë¡(ê±°ë¦¬ í° í–‰ë™ ì„ íƒ) ë³´ìƒ +1, ê·¸ ì™¸ -1, -2
    - OOC(in_control=False) : ë°˜ëŒ€ë¡œ CLì— ë” ê°€ê¹Œìš´ í–‰ë™(íƒì§€ì— ìœ ë¦¬í•œ í–‰ë™)ì„ ì„ í˜¸
    """
    # ê±°ë¦¬ê°€ í° ìˆœì„œëŒ€ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    order = np.argsort(distances)[::-1]
    # ì„ íƒí•œ action_idxì˜ ìˆœìœ„(0=ìµœìƒìœ„, 2=ìµœí•˜ìœ„)
    rank = int(np.where(order == action_idx)[0][0])

    if in_control:
        table = {0: 1.0, 1: -1.0, 2: -2.0}
    else:
        table = {0: -2.0, 1: -1.0, 2: 1.0}
    return float(table[rank])

@dataclass
class RunningStat:
    """ëŸ¬ë‹ Z-ì •ê·œí™”ë¥¼ ìœ„í•œ 1D í†µê³„ ì¶”ì ê¸°."""
    mean: float = 0.0
    var: float = 1.0
    count: int = 0

    def update(self, x: float):
        self.count += 1
        if self.count == 1:
            self.mean = x
            self.var = 1.0
            return
        # Welford
        delta = x - self.mean
        self.mean += delta / self.count
        self.var += delta * (x - self.mean)

    def z(self, x: float) -> float:
        denom = max(1e-6, float(np.sqrt(max(self.var, 1e-6))))
        return float((x - self.mean) / denom)


@dataclass
class MorlStats:
    """ê° ë³´ìƒ ì»´í¬ë„ŒíŠ¸ë³„ ëŸ°ë‹ í†µê³„."""
    sens: RunningStat = field(default_factory=RunningStat)
    stab: RunningStat = field(default_factory=RunningStat)
    shape: RunningStat = field(default_factory=RunningStat)


@dataclass
class MorlConfig:
    """
    MORL ìŠ¤ì¹¼ë¼ ë³´ìƒ ì„¤ì •.

    w_sens  : íƒì§€ ë¯¼ê°ë„(ARL1 ê°ì†Œ) ìª½ ê°€ì¤‘ì¹˜
    w_stab  : ì•ˆì •ì„±(ARL0 ìœ ì§€, false alarm íšŒí”¼) ê°€ì¤‘ì¹˜(ë³´í†µ ìŒìˆ˜ ë˜ëŠ” -ê°’ì— ê³±)
    w_shape : Algorithm1 ê¸°ë°˜ shaping ê°€ì¤‘ì¹˜
    c_detect: íƒì§€ ì„±ê³µ ì‹œ ê¸°ë³¸ ë³´ìƒ
    k_delay : íƒì§€ ì§€ì—° íŒ¨ë„í‹° ê³„ìˆ˜
    """
    w_sens: float = 1.0
    w_stab: float = 1.0
    w_shape: float = 0.3
    c_detect: float = 1.0
    k_delay: float = 0.01


@dataclass
class MorlEpisodeState:
    """ì—í”¼ì†Œë“œ ë‹¨ìœ„ë¡œ ìœ ì§€í•˜ëŠ” MORL ë‚´ë¶€ ìƒíƒœ."""
    detection_step: int | None = None   # ì²« 'íƒì§€' ì‹œì  (CLì„ ë„˜ì€ ì²« t)
    false_alarm: bool = False           # shift_time ì´ì „ì— íƒì§€ ë°œìƒ ì—¬ë¶€


def calc_reward_alg1_wrapper(
    action_global_idx: int,
    valid_indices: List[int],
    D_list: List[float],
    in_control: bool,
) -> float:
    """ê¸°ì¡´ Algorithm 1ê³¼ ì™„ì „íˆ ë™ì¼í•œ ë³´ìƒ ê³„ì‚° (fallbackìš©)."""
    if action_global_idx not in valid_indices:
        return -2.0  # ë°©ì–´ì  íŒ¨ë„í‹°
    local_idx = valid_indices.index(action_global_idx)
    return float(reward_by_algorithm1(local_idx, D_list, in_control=in_control))


def calc_reward_morl(
    *,
    action_global_idx: int,
    valid_indices: List[int],
    D_list: List[float],
    in_control: bool,
    t: int,
    shift_time: int,
    selected_D: float,
    episode_state: MorlEpisodeState,
    stats: MorlStats,
    cfg: MorlConfig,
) -> Tuple[float, Dict[str, float]]:
    """
    MORL ìŠ¤ì¹¼ë¼ ë³´ìƒ ê³„ì‚°.

    Parameters
    ----------
    action_global_idx : ì „ì²´ action_setì—ì„œì˜ ì¸ë±ìŠ¤ (0,1,2)
    valid_indices     : í˜„ì¬ ì‹œì ì—ì„œ ìœ íš¨í•œ í–‰ë™ë“¤ì˜ global index ë¦¬ìŠ¤íŠ¸
    D_list            : ìœ íš¨ í–‰ë™ë³„ D=(CL - pS0)/std ê°’ ë¦¬ìŠ¤íŠ¸ (valid_indices ìˆœì„œ)
    in_control        : í˜„ì¬ ì‹œì ì´ IC(True)ì¸ì§€ OOC(False)ì¸ì§€
    t                 : í˜„ì¬ ì‹œê°„(step)
    shift_time        : OOC ì‹œì‘ ì‹œì  (ScenarioConfig.shift_time)
    selected_D        : ì—ì´ì „íŠ¸ê°€ ì‹¤ì œë¡œ ì„ íƒí•œ í–‰ë™ì˜ D ê°’
    episode_state     : ì—í”¼ì†Œë“œ ì „ì—­ MORL ìƒíƒœ (íƒì§€/false alarm ê¸°ë¡)
    stats             : ê° ë³´ìƒ ì»´í¬ë„ŒíŠ¸ë³„ ëŸ¬ë‹ í†µê³„
    cfg               : MorlConfig (ê°€ì¤‘ì¹˜ ë“±)
    """

    # ---------------------------
    # 1) ê¸°ë³¸ shaping: Algorithm1
    # ---------------------------
    r_shape = calc_reward_alg1_wrapper(
        action_global_idx=action_global_idx,
        valid_indices=valid_indices,
        D_list=D_list,
        in_control=in_control,
    )

    # ---------------------------
    # 2) ì•ˆì •ì„± ì¸¡ë©´ (IC êµ¬ê°„)
    # ---------------------------
    # D = (CL - pS0) / std ì´ë¯€ë¡œ, Dê°€ ì‘ì„ìˆ˜ë¡ CLì— ê·¼ì ‘/ì´ˆê³¼ (ì•ŒëŒ ìœ„í—˜ â†‘)
    # IC êµ¬ê°„ì—ì„œ D_selectedê°€ 0 ê·¼ì²˜/ìŒìˆ˜ì´ë©´ false alarm ìœ„í—˜ â†’ penalty
    r_stab = 0.0
    if in_control:
        # soft penalty: D_selectedê°€ 1 ì•„ë˜ë¡œ ë‚´ë ¤ì˜¤ë©´ ì ì  ë” í° penalty
        margin = max(0.0, 1.0 - selected_D)  # selected_D <= 1 -> ì–‘ìˆ˜
        r_stab = -margin   # ì•ˆì •ì„± ê¸°ì¤€ìœ¼ë¡œëŠ” ë§ˆì§„ì´ í´ìˆ˜ë¡ ë‚˜ì¨(ìŒìˆ˜ ë³´ìƒ)

    # ---------------------------
    # 3) íƒì§€ ë¯¼ê°ë„ (OOC êµ¬ê°„)
    # ---------------------------
    r_sens = 0.0
    if (not in_control) and (not episode_state.false_alarm):
        # OOC êµ¬ê°„ì—ì„œ, D_selectedê°€ 0 ì´í•˜(ì´ë¯¸ CLì„ ë„˜ì—ˆë‹¤ê³  ê°€ì •)ë©´ "íƒì§€"ë¡œ ì²˜ë¦¬
        if selected_D <= 0.0 and episode_state.detection_step is None:
            episode_state.detection_step = t
            # detection_delay = t - shift_time (0 ì´ìƒ)
            detection_delay = max(0, t - shift_time)
            r_sens = cfg.c_detect - cfg.k_delay * float(detection_delay)
        else:
            r_sens = 0.0

    # ---------------------------
    # 4) False Alarm ì²´í¬
    # ---------------------------
    # shift_time ì´ì „ì— D_selected <= 0 ì´ë©´ False Alarm ìœ¼ë¡œ ê°„ì£¼
    if in_control and (t < shift_time) and (selected_D <= 0.0):
        episode_state.false_alarm = True

    # ---------------------------
    # 5) ëŸ¬ë‹ Z-ì •ê·œí™”
    # ---------------------------
    stats.sens.update(r_sens)
    stats.stab.update(r_stab)
    stats.shape.update(r_shape)

    z_sens = stats.sens.z(r_sens)
    z_stab = stats.stab.z(r_stab)
    z_shape = stats.shape.z(r_shape)

    # ---------------------------
    # 6) ìµœì¢… ìŠ¤ì¹¼ë¼ ë³´ìƒ (ê°€ì¤‘í•©)
    # ---------------------------
    reward = (
        cfg.w_sens * z_sens +
        cfg.w_stab * z_stab +
        cfg.w_shape * z_shape
    )

    components = {
        "Rsensitivity_raw": r_sens,
        "Pstability_raw": r_stab,
        "Rshape_raw": r_shape,
        "Rsensitivity_z": z_sens,
        "Pstability_z": z_stab,
        "Rshape_z": z_shape,
    }
    return float(reward), components

```

### `rl_pg.py`

```python
# ai_rtc/rl_pg.py
import math
from dataclasses import dataclass
from typing import List, Dict, Tuple

import numpy as np
from numpy.typing import NDArray
from sklearn.utils import check_random_state
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.optim as optim

from utils import _np2d, _np1d
from data_gen import ScenarioConfig, make_phase2_series
from cl_calib import WindowCalib
from rtc_backend import compute_pS0_stat
from reward_morl import MorlConfig, MorlStats, MorlEpisodeState, calc_reward_morl, calc_reward_alg1_wrapper



@dataclass
class RLConfig:
    action_set: Tuple[int, int, int] = (5, 10, 15)
    lr: float = 1e-3
    gamma: float = 0.99
    episodes: int = 50
    device: str = 'cpu'
    reward: str = "alg1"
    

def make_state_tensor(windowed: NDArray, d: int, L: int = 15) -> torch.Tensor:
    w = windowed.shape[0]
    if w >= L:
        data = windowed[-L:]
    else:
        pad = np.zeros((L - w, d), dtype=windowed.dtype)
        data = np.vstack([pad, windowed])
    t = torch.from_numpy(data.astype(np.float32)).unsqueeze(0).unsqueeze(0)
    return t


@dataclass
class WindowCalib:
    CL: float
    std: float
    size: int
    
def reward_by_algorithm1(action_idx: int, distances: List[float], in_control: bool) -> float:
    # distances: í° ê²ƒì´ ICì—ì„œ ì„ í˜¸ë˜ë„ë¡ ì„¤ê³„ë¨
    order = np.argsort(distances)[::-1]
    rank = int(np.where(order == action_idx)[0][0])
    return ({0: 1.0, 1: -1.0, 2: -2.0}[rank] if in_control else {0: -2.0, 1: -1.0, 2: 1.0}[rank])



def every(n, i):  # i: 0-based index
    return (i + 1) % n == 0

def train_rl_policy(
    policy: nn.Module,
    optimizer: optim.Optimizer,
    cfg: RLConfig,
    scen: ScenarioConfig,
    calib_map: Dict[int, WindowCalib],
    S0_ref: NDArray,
    seed: int,
    rf_backend: str = 'sklearn',
    n_estimators_eval: int = 150,
) -> nn.Module:
    rng = check_random_state(seed)
    policy.train()
    device = cfg.device
    if device.startswith('cuda') and torch.cuda.device_count() > 1:
        print("    > [RL] nn.DataParallel í™œì„±í™” (ë©€í‹° GPU ì‚¬ìš©)")
        policy = nn.DataParallel(policy)
    policy.to(device)
    if hasattr(torch, 'compile'):
        try:
            print("    > [RL] torch.compile() ì ìš© (PyTorch 2.x)")
            policy = torch.compile(policy)
        except Exception:
            pass
    actions = list(cfg.action_set)

    morl_cfg = MorlConfig()
    morl_stats = MorlStats()

    pbar_ep = tqdm(range(cfg.episodes), desc="[RL] Training", dynamic_ncols=True)
    for ep in pbar_ep:

        morl_ep_state = MorlEpisodeState()        
        lam_choices_I = [math.sqrt(x) for x in [0.25, 0.5, 1, 2, 3, 5, 7, 9]]
        lam_choices_II = [math.sqrt(x) for x in [2, 3, 5, 7, 9]]
        if rng.rand() < 0.5:
            scenario = 'I'; lam = float(rng.choice(lam_choices_I))
        else:
            scenario = 'II'; lam = float(rng.choice(lam_choices_II))
        X, labels_ic = make_phase2_series(scen, rng, scenario, lam)

        logps: List[torch.Tensor] = []
        rewards: List[float] = []
        a_trace: List[int] = []

        for t in range(1, scen.T + 1):
            # --- ìœ íš¨ í–‰ë™ ë§ˆìŠ¤í¬(t>=w) ---
            valid = [t >= w for w in actions]
            if not any(valid):
                # ì•„ì§ ì–´ë–¤ ì°½ë„ ê½‰ ì°¨ì§€ ëª»í•œ ì‹œì ì´ë©´ ìƒíƒœë§Œ ê°±ì‹ 
                Lmax = 15
                w_for_state = min(max(actions), t)
                state = make_state_tensor(X[t - w_for_state:t], scen.d, L=Lmax).to(device)
                with torch.no_grad():
                    _ = policy(state)
                continue

            # --- ìœ íš¨ í–‰ë™ë§Œ í†µê³„/ê±°ë¦¬ ê³„ì‚° ---
            ms_list: List[float] = []
            D_list: List[float] = []
            valid_indices: List[int] = []
            for a_idx_tmp, w in enumerate(actions):
                if not valid[a_idx_tmp]:
                    continue
                Sw = X[t - w:t]
                Xrf = np.vstack([S0_ref, Sw])
                yrf = np.hstack([np.zeros(len(S0_ref), dtype=int), np.ones(len(Sw), dtype=int)])
                Xrf = _np2d(Xrf, dtype=np.float32)
                yrf = _np1d(yrf, dtype=np.int32)
                pS0 = compute_pS0_stat(
                    Xrf, yrf, np.arange(len(S0_ref)),
                    d=scen.d, n_estimators=n_estimators_eval,
                    seed=rng.randint(1_000_000), backend=rf_backend
                )
                ms_list.append(pS0)
                calib = calib_map[w]
                D = (calib.CL - pS0) / max(1e-8, calib.std)
                D_list.append(float(D))
                valid_indices.append(a_idx_tmp)

            # --- ì •ì±… ë¶„í¬ (ìœ íš¨í•˜ì§€ ì•Šì€ í–‰ë™ ë§ˆìŠ¤í‚¹) ---
            Lmax = 15
            w_for_state = min(max(actions), t)
            state = make_state_tensor(X[t - w_for_state:t], scen.d, L=Lmax).to(device)
            logits = policy(state)
            logits_masked = logits.clone()
            for idx, ok in enumerate(valid):
                if not ok:
                    logits_masked[0, idx] = -1e9
            probs = torch.softmax(logits_masked, dim=-1)
            m = torch.distributions.Categorical(probs=probs)
            a_idx_tensor = m.sample()
            a_idx = int(a_idx_tensor.item())

            a_trace.append(actions[a_idx])
            logps.append(m.log_prob(a_idx_tensor))

            # --- ë³´ìƒ(ìœ íš¨ í–‰ë™ì˜ ë¡œì»¬ ì¸ë±ìŠ¤ ê¸°ì¤€) ---
            # --- ë³´ìƒ ê³„ì‚° ---
            ic = bool(labels_ic[t-1] == 1)

            if cfg.reward == "alg1":
                r_t = calc_reward_alg1_wrapper(
                    action_global_idx=a_idx,
                    valid_indices=valid_indices,
                    D_list=D_list,
                    in_control=ic,
                )
                rewards.append(float(r_t))
            else:  # cfg.reward == "morl"
                # ì„ íƒëœ í–‰ë™ì˜ D ê°’(selected_D) ì¶”ì¶œ
                if a_idx not in valid_indices:
                    # ì´ë¡ ìƒ ê±°ì˜ ì—†ìŒ. ë°©ì–´ì  ì²˜ë¦¬.
                    selected_D = 0.0
                else:
                    local_idx = valid_indices.index(a_idx)
                    selected_D = float(D_list[local_idx])

                r_t, comps = calc_reward_morl(
                    action_global_idx=a_idx,
                    valid_indices=valid_indices,
                    D_list=D_list,
                    in_control=ic,
                    t=t,
                    shift_time=scen.shift_time,
                    selected_D=selected_D,
                    episode_state=morl_ep_state,
                    stats=morl_stats,
                    cfg=morl_cfg,
                )
                # comps(dict)ëŠ” ì›í•˜ë©´ ë””ë²„ê·¸/ë¡œê·¸ì— í™œìš© ê°€ëŠ¥
                rewards.append(float(r_t))


        # --- Policy Gradient ì—…ë°ì´íŠ¸ ---
        pbar_ep.set_postfix_str("Updating policy.")
        G = 0.0
        returns = []
        for r in reversed(rewards):
            G = r + cfg.gamma * G
            returns.append(G)
        returns = list(reversed(returns))
        returns_t = torch.tensor(returns, dtype=torch.float32, device=device)
        if len(returns_t) > 1:
            returns_t = (returns_t - returns_t.mean()) / (returns_t.std() + 1e-8)
        loss = 0.0
        for logp, Gt in zip(logps, returns_t):
            loss = loss - logp * Gt
        optimizer.zero_grad(); loss.backward()
        torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)
        optimizer.step()

        # --- 10 ì—í”¼ì†Œë“œë§ˆë‹¤ ìš”ì•½ ë¡œê·¸ ---
        if every(10, ep):
            avg_r = float(np.mean(rewards)) if len(rewards) else 0.0
            if len(a_trace):
                vals, counts = np.unique(a_trace, return_counts=True)
                pi_hist = {int(v): f"{float(c)/float(len(a_trace)):.2f}" for v, c in zip(vals, counts)}
            else:
                pi_hist = {}
            print(f"\n[ep {ep+1}] avg_reward={avg_r:.3f}  pi(w)={pi_hist}")

    if isinstance(policy, nn.DataParallel):
        policy = policy.module
    return policy


```

### `rl_sac.py`

```python
"""
rl_sac.py

3ë‹¨ê³„: Policy Gradient â†’ Discrete SAC (í–‰ë™ = ìœˆë„ìš° í¬ê¸° {5, 10, 15} ìœ ì§€)

- í™˜ê²½/ë³´ìƒ/ìƒíƒœ êµ¬ì„±ì€ rl_pg.py ì™€ ë™ì¼í•˜ê²Œ ì‚¬ìš©
- ì •ì±… ë„¤íŠ¸ì›Œí¬ëŠ” policy_nets.py ì˜ CNN / CNN-LSTM ê·¸ëŒ€ë¡œ ì‚¬ìš©
- ì´ ëª¨ë“ˆì€ "ì•Œê³ ë¦¬ì¦˜ë§Œ" êµì²´ (off-policy SAC + ë¦¬í”Œë ˆì´ ë²„í¼ + 2x í¬ë¦¬í‹± + Î± ìë™ íŠœë‹)

í•„ìˆ˜ ì˜ì¡´ ëª¨ë“ˆ:
  - data_gen.ScenarioConfig, make_phase2_series
  - cl_calib.WindowCalib
  - rtc_backend.compute_pS0_stat
  - rl_pg.make_state_tensor, rl_pg.reward_by_algorithm1
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, Tuple, List

import numpy as np
from numpy.typing import NDArray
from sklearn.utils import check_random_state

import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical

# --- í”„ë¡œì íŠ¸ ë‚´ ëª¨ë“ˆ import (ì´ë¦„ë§Œ ë§ê²Œ ì¡°ì •í•´ì£¼ë©´ ë¨) ---
from data_gen import ScenarioConfig, make_phase2_series
from cl_calib import WindowCalib
from rtc_backend import compute_pS0_stat
from rl_pg import make_state_tensor
from reward_morl import MorlConfig, MorlStats, MorlEpisodeState, calc_reward_morl, calc_reward_alg1_wrapper, reward_by_algorithm1



# ---------------------------------------------------------------------------
# 1. SAC ì „ìš© ì„¤ì •ê°’
# ---------------------------------------------------------------------------

@dataclass
class SACConfig:
    """
    SAC ì´ì‚° ë²„ì „(Discrete SAC)ìš© ì„¤ì •ê°’.

    - action_set     : í–‰ë™ìœ¼ë¡œ ì‚¬ìš©í•  ìœˆë„ìš° í¬ê¸° íŠœí”Œ (ì˜ˆ: (5, 10, 15))
    - episodes       : í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜
    - gamma          : í• ì¸ìœ¨
    - device         : 'cuda' or 'cpu'
    - actor_lr       : ì •ì±… ë„¤íŠ¸ì›Œí¬ learning rate
    - critic_lr      : Q-ë„¤íŠ¸ì›Œí¬ learning rate
    - alpha_lr       : temperature(Î±) learning rate
    - buffer_size    : ë¦¬í”Œë ˆì´ ë²„í¼ ìµœëŒ€ í¬ê¸°
    - batch_size     : ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°
    - tau            : íƒ€ê¹ƒ ë„¤íŠ¸ì›Œí¬ soft update ê³„ìˆ˜
    - initial_random_steps : ì´ step ìˆ˜ ì „ê¹Œì§€ëŠ” ì •ì±… ëŒ€ì‹  ëœë¤ í–‰ë™ ìˆ˜í–‰
    - updates_per_step     : í™˜ê²½ì—ì„œ 1 step ìƒì„±í•  ë•Œë§ˆë‹¤ ëª‡ ë²ˆ ì—…ë°ì´íŠ¸í• ì§€
    - target_entropy       : ì—”íŠ¸ë¡œí”¼ íƒ€ê¹ƒ (ê¸°ë³¸ê°’ Noneì´ë©´ -log(#actions))
    """
    action_set: Tuple[int, int, int] = (5, 10, 15)
    episodes: int = 50
    gamma: float = 0.99
    device: str = "cpu"

    actor_lr: float = 3e-4
    critic_lr: float = 3e-4
    alpha_lr: float = 3e-4

    buffer_size: int = 100_000
    batch_size: int = 64
    tau: float = 0.005

    initial_random_steps: int = 1_000
    updates_per_step: int = 1

    target_entropy: float | None = None  # Noneì´ë©´ -log(num_actions) ë¡œ ì„¤ì •
    reward: str = "alg1"


# ---------------------------------------------------------------------------
# 2. ë¦¬í”Œë ˆì´ ë²„í¼
# ---------------------------------------------------------------------------

class ReplayBuffer:
    """
    (s, a, r, s', done, valid_mask, next_valid_mask)ë¥¼ ì €ì¥í•˜ëŠ” ê°„ë‹¨í•œ ë²„í¼.
    - state, next_state : numpy float32 ë°°ì—´ (state í…ì„œë¥¼ .cpu().numpy()ë¡œ ì €ì¥)
    - action            : int (í–‰ë™ index)
    - reward            : float
    - done              : bool
    - valid_mask        : shape (num_actions,) bool
    - next_valid_mask   : shape (num_actions,) bool
    """

    def __init__(self, capacity: int, num_actions: int):
        self.capacity = capacity
        self.num_actions = num_actions

        self.states: List[np.ndarray] = []
        self.next_states: List[np.ndarray] = []
        self.actions: List[int] = []
        self.rewards: List[float] = []
        self.dones: List[bool] = []
        self.valid_masks: List[np.ndarray] = []
        self.next_valid_masks: List[np.ndarray] = []

        self._idx = 0
        self._full = False

    def __len__(self) -> int:
        return len(self.states)

    def add(
        self,
        state: torch.Tensor,
        action: int,
        reward: float,
        next_state: torch.Tensor,
        done: bool,
        valid_mask: np.ndarray,
        next_valid_mask: np.ndarray,
    ) -> None:
        # state, next_state: (1, 1, L, d) í…ì„œë¼ê³  ê°€ì •
        s = state.detach().cpu().numpy()
        s_next = next_state.detach().cpu().numpy()

        if len(self.states) < self.capacity and not self._full:
            self.states.append(s)
            self.next_states.append(s_next)
            self.actions.append(action)
            self.rewards.append(reward)
            self.dones.append(done)
            self.valid_masks.append(valid_mask.astype(bool))
            self.next_valid_masks.append(next_valid_mask.astype(bool))
        else:
            self.states[self._idx] = s
            self.next_states[self._idx] = s_next
            self.actions[self._idx] = action
            self.rewards[self._idx] = reward
            self.dones[self._idx] = done
            self.valid_masks[self._idx] = valid_mask.astype(bool)
            self.next_valid_masks[self._idx] = next_valid_mask.astype(bool)

            self._idx = (self._idx + 1) % self.capacity
            if self._idx == 0:
                self._full = True

    def sample(self, batch_size: int, device: str):
        idxs = np.random.randint(0, len(self.states), size=batch_size)

        states = torch.from_numpy(np.concatenate([self.states[i] for i in idxs], axis=0)).to(device)
        next_states = torch.from_numpy(np.concatenate([self.next_states[i] for i in idxs], axis=0)).to(device)
        actions = torch.tensor([self.actions[i] for i in idxs], dtype=torch.long, device=device)
        rewards = torch.tensor([self.rewards[i] for i in idxs], dtype=torch.float32, device=device)
        dones = torch.tensor([self.dones[i] for i in idxs], dtype=torch.float32, device=device)
        valid_masks = torch.from_numpy(np.stack([self.valid_masks[i] for i in idxs])).to(device)
        next_valid_masks = torch.from_numpy(np.stack([self.next_valid_masks[i] for i in idxs])).to(device)

        return states, actions, rewards, next_states, dones, valid_masks, next_valid_masks


# ---------------------------------------------------------------------------
# 3. Q-ë„¤íŠ¸ì›Œí¬(í¬ë¦¬í‹±) ì •ì˜
# ---------------------------------------------------------------------------

class QNetwork(nn.Module):
    """
    ê°„ë‹¨í•œ MLP ê¸°ë°˜ Q ë„¤íŠ¸ì›Œí¬.
    - ì…ë ¥: state í…ì„œ (B, 1, L, d)
    - ì¶œë ¥: Q(s, a) ë²¡í„° (B, num_actions)

    ì •ì±… CNN/CNN-LSTM ê³¼ëŠ” ë³„ë„ë¡œ, ì—¬ê¸°ì„œëŠ” ìƒíƒœë¥¼ ë‹¨ìˆœíˆ flatten í•´ì„œ ì‚¬ìš©.
    """

    def __init__(self, num_actions: int, L: int = 15, d: int = 10, hidden_dim: int = 256):
        super().__init__()
        self.num_actions = num_actions
        self.L = L
        self.d = d

        in_dim = L * d   # (1, 1, L, d) -> flatten
        self.net = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, num_actions),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B, 1, L, d)
        if x.dim() != 4:
            raise ValueError(f"QNetwork expects (B,1,L,d) but got {x.shape}")
        b, c, L, d = x.shape
        x = x.view(b, -1)   # (B, L*d)
        return self.net(x)


# ---------------------------------------------------------------------------
# 4. SAC í•™ìŠµ ë£¨í”„
# ---------------------------------------------------------------------------

@torch.no_grad()
def _select_action_with_mask(
    policy: nn.Module,
    state: torch.Tensor,
    valid_mask: np.ndarray,
    device: str,
    rng: np.random.RandomState,
    step: int,
    cfg: SACConfig,
) -> Tuple[int, torch.Tensor]:
    """
    - policy(state) â†’ logits
    - valid_mask=False ì¸ í–‰ë™ì€ -1e9ë¡œ ë§ˆìŠ¤í‚¹ í›„ softmax
    - cfg.initial_random_steps ì´ì „ì—ëŠ” ëœë¤(ìœ íš¨ í–‰ë™ì—ì„œë§Œ) ì„ íƒ
    - ë°˜í™˜ê°’: (action_index, probs_tensor)   # probs_tensor: (1, num_actions)
    """
    num_actions = len(valid_mask)
    valid_mask_t = torch.from_numpy(valid_mask.astype(bool)).to(device=device)

    logits = policy(state.to(device))  # (1, num_actions)
    logits = logits.clone()
    # valid_mask_t: (num_actions,) -> (1, num_actions)ì— ë¸Œë¡œë“œìºìŠ¤íŒ…
    logits[:, ~valid_mask_t] = -1e9

    if step < cfg.initial_random_steps:
        # ìœ íš¨í•œ í–‰ë™ ì¤‘ ëœë¤
        valid_indices = np.where(valid_mask)[0]
        if len(valid_indices) == 0:
            # ì´ë¡ ìƒ ì—¬ê¸° ì˜¤ì§€ ì•Šë„ë¡ í•™ìŠµ ë£¨í”„ì—ì„œ ì²˜ë¦¬
            a_idx = int(rng.randint(0, num_actions))
        else:
            a_idx = int(rng.choice(valid_indices))
        probs = torch.softmax(logits, dim=-1)
        return a_idx, probs

    probs = torch.softmax(logits, dim=-1)
    m = Categorical(probs=probs)
    a_idx_tensor = m.sample()
    a_idx = int(a_idx_tensor.item())
    return a_idx, probs


def train_sac_policy(
    policy: nn.Module,
    sac_cfg: SACConfig,
    scen: ScenarioConfig,
    calib_map: Dict[int, WindowCalib],
    S0_ref: NDArray,
    seed: int,
    rf_backend: str = "sklearn",
    n_estimators_eval: int = 150,
) -> nn.Module:
    """
    Discrete SAC ìœ¼ë¡œ ì •ì±… ë„¤íŠ¸ì›Œí¬ë¥¼ í•™ìŠµ.

    ì¸ì:
      - policy      : policy_nets.build_policy() ë¡œ ë§Œë“  CNN ë˜ëŠ” CNN-LSTM
      - sac_cfg     : SACConfig
      - scen        : ScenarioConfig (data_gen.py)
      - calib_map   : {window_size: WindowCalib(CL, std, ...)}
      - S0_ref      : Phase I ì°¸ì¡° ë°ì´í„° (ì •ìƒêµ°)
      - seed        : random seed
      - rf_backend  : 'sklearn' / 'lgbm' ë“± RTC ë¶„ë¥˜ê¸° backend
    ë°˜í™˜:
      - í•™ìŠµëœ policy (in-place update)
    """
    # ì „ì—­ MORL í†µê³„ (ì—í”¼ì†Œë“œ ì „ì²´ ê³µìœ )
    morl_cfg = MorlConfig()
    morl_stats = MorlStats()    
    
    rng = check_random_state(seed)
    device = sac_cfg.device

    policy.to(device)
    policy.train()

    num_actions = len(sac_cfg.action_set)
    Lmax = 15
    d = scen.d

    # --- Q ë„¤íŠ¸ì›Œí¬ & íƒ€ê¹ƒ ë„¤íŠ¸ì›Œí¬ ---
    q1 = QNetwork(num_actions=num_actions, L=Lmax, d=d).to(device)
    q2 = QNetwork(num_actions=num_actions, L=Lmax, d=d).to(device)
    q1_target = QNetwork(num_actions=num_actions, L=Lmax, d=d).to(device)
    q2_target = QNetwork(num_actions=num_actions, L=Lmax, d=d).to(device)
    q1_target.load_state_dict(q1.state_dict())
    q2_target.load_state_dict(q2.state_dict())

    # --- Optimizers ---
    actor_optimizer = optim.Adam(policy.parameters(), lr=sac_cfg.actor_lr)
    critic_optimizer = optim.Adam(
        list(q1.parameters()) + list(q2.parameters()),
        lr=sac_cfg.critic_lr,
    )

    # --- Temperature Î± (ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜) ---
    log_alpha = torch.zeros(1, requires_grad=True, device=device)
    alpha_optimizer = optim.Adam([log_alpha], lr=sac_cfg.alpha_lr)
    if sac_cfg.target_entropy is None:
        target_entropy = -float(np.log(num_actions))
    else:
        target_entropy = sac_cfg.target_entropy

    # --- Replay Buffer ---
    buffer = ReplayBuffer(capacity=sac_cfg.buffer_size, num_actions=num_actions)

    global_step = 0

    from tqdm import tqdm
    pbar_ep = tqdm(range(sac_cfg.episodes), desc="[RL-SAC] Training", dynamic_ncols=True)

    actions = list(sac_cfg.action_set)

    for ep in pbar_ep:
        # --- ì‹œë‚˜ë¦¬ì˜¤/ì‰¬í”„íŠ¸ í¬ê¸° ìƒ˜í”Œë§ (PG ì½”ë“œì™€ ë™ì¼ íŒ¨í„´) ---
        morl_ep_state = MorlEpisodeState()
        
        lam_choices_I = [np.sqrt(x) for x in [0.25, 0.5, 1, 2, 3, 5, 7, 9]]
        lam_choices_II = [np.sqrt(x) for x in [2, 3, 5, 7, 9]]

        if rng.rand() < 0.5:
            scenario = "I"
            lam = float(rng.choice(lam_choices_I))
        else:
            scenario = "II"
            lam = float(rng.choice(lam_choices_II))

        # Phase II ì‹œê³„ì—´ ìƒì„±
        X, labels_ic = make_phase2_series(scen, rng, scenario, lam)  # X: (T, d)

        ep_rewards: List[float] = []

        for t in range(1, scen.T + 1):
            global_step += 1

            # --- ìœ íš¨ í–‰ë™ ë§ˆìŠ¤í¬(t>=w) ---
            valid = np.array([t >= w for w in actions], dtype=bool)
            if not valid.any():
                # ì•„ì§ ì–´ë–¤ ì°½ë„ ê½‰ ì°¨ì§€ ëª»í•œ ì‹œì ì´ë©´ ìƒíƒœë§Œ policyì— ë„£ê³  í†µê³¼
                w_for_state = min(max(actions), t)
                state = make_state_tensor(X[t - w_for_state:t], d, L=Lmax).to(device)
                _ = policy(state)
                continue

            # --- ìƒíƒœ êµ¬ì„± ---
            w_for_state = min(max(actions), t)
            state = make_state_tensor(X[t - w_for_state:t], d, L=Lmax).to(device)

            # --- ìœ íš¨ í–‰ë™ë“¤ì— ëŒ€í•´ í†µê³„/ê±°ë¦¬ ê³„ì‚° ---
            ms_list: List[float] = []
            D_list: List[float] = []
            valid_indices: List[int] = []

            for a_idx_tmp, w in enumerate(actions):
                if not valid[a_idx_tmp]:
                    continue
                Sw = X[t - w:t]
                Xrf = np.vstack([S0_ref, Sw])
                yrf = np.hstack([
                    np.zeros(len(S0_ref), dtype=int),
                    np.ones(len(Sw), dtype=int),
                ])

                pS0 = compute_pS0_stat(
                    Xrf,
                    yrf,
                    np.arange(len(S0_ref)),
                    d=scen.d,
                    n_estimators=n_estimators_eval,
                    seed=rng.randint(1_000_000),
                    backend=rf_backend,
                )
                ms_list.append(pS0)
                calib = calib_map[w]
                D = (calib.CL - pS0) / max(1e-8, calib.std)
                D_list.append(float(D))
                valid_indices.append(a_idx_tmp)

            # --- í–‰ë™ ì„ íƒ (SAC ì •ì±… or ëœë¤) ---
            a_idx, _ = _select_action_with_mask(
                policy=policy,
                state=state,
                valid_mask=valid,
                device=device,
                rng=rng,
                step=global_step,
                cfg=sac_cfg,
            )

            # --- ë³´ìƒ ê³„ì‚°  ---
            ic = bool(labels_ic[t - 1] == 1)

            if sac_cfg.reward == "alg1":
                reward = calc_reward_alg1_wrapper(
                    action_global_idx=a_idx,
                    valid_indices=valid_indices,
                    D_list=D_list,
                    in_control=ic,
                )
            else:  # 'morl'
                if a_idx not in valid_indices:
                    selected_D = 0.0
                else:
                    local_idx = valid_indices.index(a_idx)
                    selected_D = float(D_list[local_idx])

                reward, comps = calc_reward_morl(
                    action_global_idx=a_idx,
                    valid_indices=valid_indices,
                    D_list=D_list,
                    in_control=ic,
                    t=t,
                    shift_time=scen.shift_time,
                    selected_D=selected_D,
                    episode_state=morl_ep_state,
                    stats=morl_stats,
                    cfg=morl_cfg,
                )
            ep_rewards.append(float(reward))


            # --- ë‹¤ìŒ ìƒíƒœ êµ¬ì„± ---
            done = (t == scen.T)
            t_next = min(t + 1, scen.T)
            w_for_next_state = min(max(actions), t_next)
            next_state = make_state_tensor(X[t_next - w_for_next_state:t_next], d, L=Lmax).to(device)

            valid_next = np.array([t_next >= w for w in actions], dtype=bool)

            # --- ë²„í¼ì— transition ì¶”ê°€ ---
            buffer.add(
                state=state,
                action=a_idx,
                reward=reward,
                next_state=next_state,
                done=done,
                valid_mask=valid,
                next_valid_mask=valid_next,
            )

            # --- enough data: SAC ì—…ë°ì´íŠ¸ ---
            if len(buffer) >= sac_cfg.batch_size:
                for _ in range(sac_cfg.updates_per_step):
                    (
                        states_b,
                        actions_b,
                        rewards_b,
                        next_states_b,
                        dones_b,
                        valid_masks_b,
                        next_valid_masks_b,
                    ) = buffer.sample(sac_cfg.batch_size, device=device)

                    # -----------------------------
                    # 1) Critic ì—…ë°ì´íŠ¸
                    # -----------------------------
                    with torch.no_grad():
                        # next-state ì—ì„œ ì •ì±… ë¶„í¬ ê³„ì‚° + ë§ˆìŠ¤í‚¹
                        next_logits = policy(next_states_b)               # (B, num_actions)
                        next_logits = next_logits.clone()
                        next_logits = next_logits.masked_fill(~next_valid_masks_b, -1e9)


                        next_log_probs = torch.log_softmax(next_logits, dim=-1)
                        next_probs = torch.softmax(next_logits, dim=-1)

                        q1_next = q1_target(next_states_b)
                        q2_next = q2_target(next_states_b)
                        min_q_next = torch.min(q1_next, q2_next)

                        alpha = log_alpha.exp()
                        # V(s') = E_a[ Q(s',a) - Î± log Ï€(a|s') ]
                        next_v = (next_probs * (min_q_next - alpha * next_log_probs)).sum(dim=-1)

                        target_q = rewards_b + sac_cfg.gamma * (1.0 - dones_b) * next_v

                    q1_values = q1(states_b).gather(1, actions_b.unsqueeze(-1)).squeeze(-1)
                    q2_values = q2(states_b).gather(1, actions_b.unsqueeze(-1)).squeeze(-1)

                    critic_loss = ((q1_values - target_q) ** 2 + (q2_values - target_q) ** 2).mean()

                    critic_optimizer.zero_grad()
                    critic_loss.backward()
                    nn.utils.clip_grad_norm_(list(q1.parameters()) + list(q2.parameters()), 1.0)
                    critic_optimizer.step()

                    # -----------------------------
                    # 2) Actor(ì •ì±…) ì—…ë°ì´íŠ¸
                    # -----------------------------
                    logits = policy(states_b)        # (B, num_actions)
                    logits = logits.clone()
                    logits = logits.masked_fill(~valid_masks_b, -1e9)

                    log_probs = torch.log_softmax(logits, dim=-1)
                    probs = torch.softmax(logits, dim=-1)

                    q1_pi = q1(states_b)
                    q2_pi = q2(states_b)
                    min_q_pi = torch.min(q1_pi, q2_pi)

                    alpha = log_alpha.exp()
                    actor_loss = (probs * (alpha * log_probs - min_q_pi)).sum(dim=-1).mean()

                    actor_optimizer.zero_grad()
                    actor_loss.backward()
                    nn.utils.clip_grad_norm_(policy.parameters(), 1.0)
                    actor_optimizer.step()

                    # -----------------------------
                    # 3) Î± ì—…ë°ì´íŠ¸ (ì—”íŠ¸ë¡œí”¼ íƒ€ê¹ƒ)
                    # -----------------------------
                    entropy = -(probs * log_probs).sum(dim=-1).mean()
                    alpha_loss = -(log_alpha * (target_entropy - entropy).detach()).mean()

                    alpha_optimizer.zero_grad()
                    alpha_loss.backward()
                    alpha_optimizer.step()

                    # -----------------------------
                    # 4) íƒ€ê¹ƒ ë„¤íŠ¸ì›Œí¬ soft update
                    # -----------------------------
                    with torch.no_grad():
                        for param, target_param in zip(q1.parameters(), q1_target.parameters()):
                            target_param.data.mul_(1.0 - sac_cfg.tau)
                            target_param.data.add_(sac_cfg.tau * param.data)

                        for param, target_param in zip(q2.parameters(), q2_target.parameters()):
                            target_param.data.mul_(1.0 - sac_cfg.tau)
                            target_param.data.add_(sac_cfg.tau * param.data)

        # --- ì—í”¼ì†Œë“œ ë¡œê·¸ ---
        if len(ep_rewards):
            avg_r = float(np.mean(ep_rewards))
        else:
            avg_r = 0.0
        pbar_ep.set_postfix_str(f"avg_r={avg_r:.3f}, buffer={len(buffer)}")

    return policy

```

### `rtc_backend.py`

```python
import os
from typing import Literal
import numpy as np
from numpy.typing import NDArray
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils import check_random_state

from utils import sqrt_int, _np2d, _np1d

# -------------------------- p(S0,t) ê³„ì‚° ----------------------------

BackendName = Literal["sklearn", "cuml_cv", "lgbm"]

def compute_pS0_stat(
    X: NDArray,
    y: NDArray,
    idx_S0: NDArray,
    d: int,
    n_estimators: int,
    seed: int,
    backend: BackendName = "sklearn",
    kfold: int = 5,
) -> float:
    if backend == 'sklearn':
        rf = RandomForestClassifier(
            n_estimators=n_estimators,
            max_features=sqrt_int(d),
            bootstrap=True,
            oob_score=True,
            n_jobs=-1,
            random_state=seed,
        )
        rf.fit(X, y)
        oob = getattr(rf, 'oob_decision_function_', None)
        if oob is None:
            # ìƒ˜í”Œì´ ë„ˆë¬´ ì ì–´ oobê°€ ì—†ìœ¼ë©´ ì•ˆì „ fallback
            X_S0_np = X[idx_S0]
            if X_S0_np.ndim == 1:
                X_S0_np = X_S0_np.reshape(1, -1)
            proba = rf.predict_proba(X_S0_np)
            return float(np.mean(proba[:, 0]))
        p0 = oob[idx_S0, 0]
        return float(np.nanmean(p0))

    elif backend == 'lgbm':
        # LightGBM OOF í™•ë¥  ê¸°ë°˜ p(S0,t) ì¶”ì •
        try:
            from sklearn.model_selection import StratifiedKFold
            from lightgbm import LGBMClassifier
        except Exception as e:
            raise RuntimeError("LightGBMì´ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ backend='lgbm'ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. pip install lightgbm") from e
        
        use_gpu = os.environ.get("AI_RTC_DEVICE", "cpu").lower() == "cuda"

        # âœ… 1) ë¬´ì¡°ê±´ ë„˜íŒŒì´ë¡œ ìºìŠ¤íŒ… (DataFrame â†’ ndarray)
        #    astype(...)ëŠ” DFë¥¼ ì—¬ì „íˆ DFë¡œ ìœ ì§€í•˜ë¯€ë¡œ ê²½ê³  ì›ì¸ì´ ë¨
        X_np = np.asarray(X, dtype=np.float32)
        y_np = np.asarray(y, dtype=np.int32)

        # (ì„ íƒ) ë©”ëª¨ë¦¬ ì—°ì†ì„± í™•ë³´ â€“ ì¼ë¶€ í™˜ê²½ì—ì„œ ì•½ê°„ì˜ ì´ë“
        X_np = np.ascontiguousarray(X_np)
        y_np = np.ascontiguousarray(y_np)

        # --- í•µì‹¬ ê°€ë“œ: ì†Œìˆ˜ í´ë˜ìŠ¤ ê°œìˆ˜ì— ë§ì¶° n_splitsë¥¼ ì•ˆì „í•˜ê²Œ ì¡°ì • ---
        class_counts = np.bincount(y_np, minlength=2)
        min_class = int(class_counts.min())

        # --- Case A: ì†Œìˆ˜ í´ë˜ìŠ¤<2 â†’ CV ë¶ˆê°€ â†’ ê³µì •í•œ ë°±ì—…(sklearn OOB) ---
        if min_class < 2:
            from sklearn.ensemble import RandomForestClassifier as skRF
            sk = skRF(
                n_estimators=n_estimators,
                max_features=max(1, int(np.sqrt(X_np.shape[1]))),
                bootstrap=True,
                oob_score=True,
                random_state=seed,
                n_jobs=-1,
            )
            sk.fit(X_np, y_np)
            oob = getattr(sk, "oob_decision_function_", None)
            if oob is not None:
                return float(np.nanmean(oob[idx_S0, 0]))
            X_S0_np = X_np[idx_S0]
            if X_S0_np.ndim == 1:
                X_S0_np = X_S0_np.reshape(1, -1)
            proba = sk.predict_proba(X_S0_np)
            return float(np.mean(proba[:, 0]))

        # --- Case B: CV ê°€ëŠ¥ â†’ StratifiedKFoldë¡œ ì•ˆì •í™” ---
        try:
            n_splits = max(2, min(kfold, min_class))  # ì™¸ë¶€ kfold ì„¤ì •ì´ ìˆìœ¼ë©´ ì‚¬ìš©
        except NameError:
            n_splits = max(3, min(5, min_class))      # ë°±ì—… ë””í´íŠ¸(3~5)

        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)

        # ë¶ˆê· í˜• ì²˜ë¦¬ ê°€ì¤‘ì¹˜: scale_pos_weight = (#neg / #pos)
        n_pos = int((y_np == 1).sum())
        n_neg = int((y_np == 0).sum())
        scale_pos_weight = float(n_neg) / float(max(1, n_pos))

        # âœ… 2) scikit API ì‚¬ìš© (fit/predict ëª¨ë‘ ndarrayë¡œ í†µì¼)
        lgbm_params = dict(
            objective="binary",
            boosting_type="gbdt",
            n_estimators=200,          # RFì™€ ë¶„ë¦¬ëœ í•©ë¦¬ì  preset
            num_leaves=31,
            learning_rate=0.05,
            subsample=0.9,
            colsample_bytree=0.9,
            reg_lambda=1.0,
            random_state=seed,
            n_jobs=-1,
            verbose=-1,
            scale_pos_weight=max(1.0, scale_pos_weight),
        )
        
        lgbm_params["device_type"] = "gpu" if use_gpu else "cpu"

        p0_vals = []
        for tr_idx, te_idx in skf.split(X_np, y_np):
            y_tr_np = y_np[tr_idx]
            if np.unique(y_tr_np).size < 2:
                continue  # í•™ìŠµì„¸íŠ¸ê°€ ë‹¨ì¼í´ë˜ìŠ¤ë©´ ê±´ë„ˆëœ€

            model = LGBMClassifier(**lgbm_params)
            model.fit(X_np[tr_idx], y_tr_np)  # âœ… fit: ndarray

            # í…ŒìŠ¤íŠ¸ í´ë“œ ì¤‘ S0 ìœ„ì¹˜ë§Œ í™•ë¥  ì·¨í•©
            te_S0_idx = te_idx[np.isin(te_idx, idx_S0)]
            if te_S0_idx.size == 0:
                continue

            X_te_np = X_np[te_S0_idx]
            if X_te_np.ndim == 1:
                X_te_np = X_te_np.reshape(1, -1)

            proba = model.predict_proba(X_te_np)  # âœ… predict: ndarray
            p0_vals.append(proba[:, 0])           # class-0 í™•ë¥ ì´ p(S0,t)

            # ë©”ëª¨ë¦¬ ì •ë¦¬(í° ì‹¤í—˜ì—ì„œ ìœ ìš©)
            try:
                del model, proba
            except Exception:
                pass

        if not p0_vals:
            # ê·¹ë‹¨ ì¼€ì´ìŠ¤: ëª¨ë“  í´ë“œ ìŠ¤í‚µ â†’ sklearn OOBë¡œ ë°±ì—…
            from sklearn.ensemble import RandomForestClassifier as skRF
            sk = skRF(
                n_estimators=n_estimators,
                max_features=max(1, int(np.sqrt(X_np.shape[1]))),
                bootstrap=True,
                oob_score=True,
                random_state=seed,
                n_jobs=-1,
            )
            sk.fit(X_np, y_np)
            oob = getattr(sk, "oob_decision_function_", None)
            if oob is not None:
                return float(np.nanmean(oob[idx_S0, 0]))
            X_S0_np = X_np[idx_S0]
            if X_S0_np.ndim == 1:
                X_S0_np = X_S0_np.reshape(1, -1)
            proba = sk.predict_proba(X_S0_np)
            return float(np.mean(proba[:, 0]))

        return float(np.mean(np.concatenate(p0_vals)))

    elif backend == 'cuml_cv':
        try:
            import cupy as cp
            from cuml.ensemble import RandomForestClassifier as cuRF
            from sklearn.model_selection import StratifiedKFold
            mempool = cp.get_default_memory_pool() # <-- (ê¸°ì¡´ ìˆ˜ì •ì•ˆì— ì´ë¯¸ ìˆì–´ì•¼ í•¨)
        except Exception as e:
            raise RuntimeError("cuMLì´ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ backend='cuml_cv'ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. condaë¡œ RAPIDSë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.") from e

        X_np = X.astype(np.float32)
        y_np = y.astype(np.int32)

        # --- í•µì‹¬ ê°€ë“œ: ì†Œìˆ˜ í´ë˜ìŠ¤ ê°œìˆ˜ì— ë§ì¶° n_splitsë¥¼ ì•ˆì „í•˜ê²Œ ì¡°ì • ---
        class_counts = np.bincount(y_np, minlength=2)
        min_class = int(class_counts.min())

        # --- Case A: ì†Œìˆ˜ í´ë˜ìŠ¤<2 â†’ CV ë¶ˆê°€ â†’ ê³µì •í•œ ë°±ì—…(sklearn OOB) ---
        if min_class < 2:
            from sklearn.ensemble import RandomForestClassifier as skRF
            sk = skRF(
                n_estimators=n_estimators,
                max_features=max(1, int(np.sqrt(X_np.shape[1]))),
                bootstrap=True,
                oob_score=True,
                random_state=seed,
                n_jobs=-1,
            )
            sk.fit(X_np, y_np)
            oob = getattr(sk, "oob_decision_function_", None)
            if oob is not None:
                p0 = oob[idx_S0, 0]
                return float(np.nanmean(p0))
            X_S0_np = X_np[idx_S0]
            if X_S0_np.ndim == 1:
                X_S0_np = X_S0_np.reshape(1, -1)
            proba = sk.predict_proba(X_S0_np)
            return float(np.mean(proba[:, 0]))

        # --- Case B: CV ê°€ëŠ¥ â†’ StratifiedKFoldë¡œ ì•ˆì •í™” ---
        n_splits = max(2, min(kfold, min_class))
        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)

        p0_vals = []
        for tr_idx, te_idx in skf.split(X_np, y_np):
            y_tr_np = y_np[tr_idx]
            if np.unique(y_tr_np).size < 2:
                continue  # í•™ìŠµì„¸íŠ¸ê°€ ë‹¨ì¼í´ë˜ìŠ¤ë©´ ê±´ë„ˆëœ€
            # í•™ìŠµ(cuML)
            X_tr = cp.asarray(X_np[tr_idx]); y_tr = cp.asarray(y_tr_np)
            model = cuRF(
                n_estimators=n_estimators,
                max_features=max(1, int(np.sqrt(X_np.shape[1]))),
                bootstrap=True,
                n_streams=1,  # ì¬í˜„ì„± ê°•í™”
                random_state=seed,
            )
            model.fit(X_tr, y_tr)

            # í…ŒìŠ¤íŠ¸ì—ì„œ S0 ìœ„ì¹˜ë§Œ í™•ë¥  ì·¨í•©
            te_S0_mask = np.isin(te_idx, idx_S0)
            te_S0_idx = te_idx[te_S0_mask]
            if te_S0_idx.size == 0:
                continue
            X_te_np = X_np[te_S0_idx]
            if X_te_np.ndim == 1:
                X_te_np = X_te_np.reshape(1, -1)
            X_te = cp.asarray(X_te_np)
            proba = model.predict_proba(X_te)
            p0_vals.append(cp.asnumpy(proba)[:, 0])

# --- [ìˆ˜ì •] VRAM ëˆ„ìˆ˜ ë°©ì§€ë¥¼ ìœ„í•œ ëª…ì‹œì  ë©”ëª¨ë¦¬ í•´ì œ (íŒ¨ì¹˜ C) ---
            try:
                del model, X_tr, y_tr, X_te, proba
            except Exception:
                pass
            try:
                mempool.free_all_blocks()
                cp.get_default_pinned_memory_pool().free_all_blocks()
            except Exception:
                pass
            # -----------------------------------------------------


        if not p0_vals:
            # ê·¹ë‹¨ ì¼€ì´ìŠ¤: ëª¨ë“  í´ë“œ ìŠ¤í‚µ â†’ sklearn OOBë¡œ ë°±ì—…
            from sklearn.ensemble import RandomForestClassifier as skRF
            sk = skRF(
                n_estimators=n_estimators,
                max_features=max(1, int(np.sqrt(X_np.shape[1]))),
                bootstrap=True,
                oob_score=True,
                random_state=seed,
                n_jobs=-1,
            )
            sk.fit(X_np, y_np)
            oob = getattr(sk, "oob_decision_function_", None)
            if oob is not None:
                return float(np.nanmean(oob[idx_S0, 0]))
            X_S0_np = X_np[idx_S0]
            if X_S0_np.ndim == 1:
                X_S0_np = X_S0_np.reshape(1, -1)
            proba = sk.predict_proba(X_S0_np)
            return float(np.mean(proba[:, 0]))

        return float(np.mean(np.concatenate(p0_vals)))

    else:
        raise ValueError(f"Unknown backend: {backend}")


```

### `runner.py`

```python
# ai_rtc/runner.py

from __future__ import annotations

import os
import math
import pickle
import csv

from datetime import datetime
from typing import Dict

import numpy as np
from numpy.typing import NDArray
from sklearn.utils import check_random_state
import torch
import torch.optim as optim
from tqdm import tqdm

from config import build_arg_parser, config_from_args, MainConfig
from utils import set_seed
from data_gen import ScenarioConfig, gen_reference_data
from cl_calib import estimate_CL_for_window, WindowCalib
from policy_nets import build_policy, save_policy, load_policy
from rl_pg import RLConfig, train_rl_policy
from eval_arl import evaluate_arl1, evaluate_arl0
from benchmark import run_backend_benchmark  

from rl_pg import train_rl_policy
from rl_sac import train_sac_policy, SACConfig


def _prepare_phase1_data(cfg: MainConfig, scen: ScenarioConfig, rng) -> NDArray:
    """S0_ref (Phase I ê¸°ì¤€ ë°ì´í„°)ë¥¼ ë¶ˆëŸ¬ì˜¤ê±°ë‚˜ ìƒˆë¡œ ìƒì„±."""
    if cfg.S0_ref_path and os.path.exists(cfg.S0_ref_path):
        print(f"[Phase I] ê¸°ì¡´ S0_ref ë¡œë“œ: {cfg.S0_ref_path}")
        S0_ref = np.load(cfg.S0_ref_path)
    else:
        print("[Phase I] S0_ref ìƒˆë¡œ ìƒì„± ì¤‘...")
        S0_ref = gen_reference_data(scen, rng)
        if cfg.S0_ref_path:
            os.makedirs(os.path.dirname(cfg.S0_ref_path), exist_ok=True)
            np.save(cfg.S0_ref_path, S0_ref)
            print(f"[Phase I] S0_ref ì €ì¥: {cfg.S0_ref_path}")
    return S0_ref


def _prepare_cl_calib(
    cfg: MainConfig,
    scen: ScenarioConfig,
    S0_ref: NDArray,
) -> Dict[int, WindowCalib]:
    """ìœˆë„ìš°ë³„ CL ë³´ì • ìˆ˜í–‰ í˜¹ì€ ê¸°ì¡´ calib_map ë¡œë“œ."""
    action_set = cfg.action_set

    if cfg.calib_map_path and os.path.exists(cfg.calib_map_path) and cfg.n_boot == 0:
        print(f"[CL] n_boot=0 & ê¸°ì¡´ calib_map ì‚¬ìš©: {cfg.calib_map_path}")
        with open(cfg.calib_map_path, "rb") as f:
            calib_map = pickle.load(f)
        return calib_map

    print(f"[CL] ë¶€íŠ¸ìŠ¤íŠ¸ë©ìœ¼ë¡œ CL ì¶”ì • ì‹œì‘ (n_boot={cfg.n_boot})")
    calib_map: Dict[int, WindowCalib] = {}
    for w in tqdm(action_set, desc="[CL] windowë³„ CL ì¶”ì •"):
        calib = estimate_CL_for_window(
            S0_ref,
            d=scen.d,
            window=w,
            n_boot=cfg.n_boot,
            n_estimators=cfg.n_estimators_eval,
            seed=cfg.seed,
            backend=cfg.rf_backend,
            target_arl0=cfg.target_arl0,
        )
        calib_map[w] = calib

    if cfg.calib_map_path:
        os.makedirs(os.path.dirname(cfg.calib_map_path), exist_ok=True)
        with open(cfg.calib_map_path, "wb") as f:
            pickle.dump(calib_map, f)
        print(f"[CL] calib_map ì €ì¥: {cfg.calib_map_path}")

    return calib_map


def _train_policy(
    cfg: MainConfig,
    scen: ScenarioConfig,
    calib_map: Dict[int, WindowCalib],
    S0_ref: NDArray,
) -> torch.nn.Module:
    """ì •ì±… ë„¤íŠ¸ì›Œí¬ ìƒì„± + RL í•™ìŠµ (PG ë˜ëŠ” SAC)."""
    device = cfg.device
    action_set = cfg.action_set

    # -------------------------------
    # 1) ì •ì±… ë„¤íŠ¸ì›Œí¬ ìƒì„±
    # -------------------------------
    policy = build_policy(
        cfg.policy_arch,
        d=scen.d,
        num_actions=len(action_set)
    )
    policy.to(device)

    # -------------------------------
    # 2) ê¸°ì¡´ ê°€ì¤‘ì¹˜ ë¡œë“œ (ì„ íƒ)
    # -------------------------------
    if cfg.policy_in and os.path.exists(cfg.policy_in):
        print(f"[RL] ê¸°ì¡´ ì •ì±… ë¡œë“œ: {cfg.policy_in}")
        policy = load_policy(
            path=cfg.policy_in,
            d=scen.d,
            num_actions=len(action_set),
            device=device,
            arch=cfg.policy_arch,
        )

    # -------------------------------
    # 3) RL í•™ìŠµ (PG ë˜ëŠ” SAC)
    # -------------------------------
    if cfg.algo == "pg":
        # ê¸°ì¡´ PG ê²½ë¡œ
        rl_cfg = RLConfig(
            action_set=action_set,
            episodes=cfg.episodes,
            device=device,
            reward=cfg.reward,
        )
        optimizer = optim.Adam(policy.parameters(), lr=cfg.rl_lr)

        print(f"[RL] Policy Gradient í•™ìŠµ ì‹œì‘ (episodes={cfg.episodes})")

        policy = train_rl_policy(
            policy=policy,
            optimizer=optimizer,
            cfg=rl_cfg,
            scen=scen,
            calib_map=calib_map,
            S0_ref=S0_ref,
            seed=cfg.seed,
            rf_backend=cfg.rf_backend,
            n_estimators_eval=cfg.n_estimators_eval,
        )

    elif cfg.algo == "sac_discrete":
        # SAC ì´ì‚° ê²½ë¡œ
        print(f"[RL] SAC(Discrete) í•™ìŠµ ì‹œì‘ (episodes={cfg.episodes})")

        sac_cfg = SACConfig(
            action_set=tuple(cfg.action_set),
            episodes=cfg.episodes,
            device=device,
            reward=cfg.reward,
        )

        policy = train_sac_policy(
            policy=policy,
            sac_cfg=sac_cfg,
            scen=scen,
            calib_map=calib_map,
            S0_ref=S0_ref,
            seed=cfg.seed,
            rf_backend=cfg.rf_backend,
            n_estimators_eval=cfg.n_estimators_eval,
        )

    else:
        raise ValueError(f"Unknown cfg.algo '{cfg.algo}'. (ì§€ì›: 'pg', 'sac_discrete')")

    # -------------------------------
    # 4) í•™ìŠµëœ ì •ì±… ì €ì¥ (ì„ íƒ)
    # -------------------------------
    if cfg.policy_out:
        os.makedirs(os.path.dirname(cfg.policy_out), exist_ok=True)
        save_policy(policy, cfg.policy_out)
        print(f"[RL] í•™ìŠµëœ ì •ì±… ì €ì¥: {cfg.policy_out}")

    return policy


def _evaluate(
    cfg: MainConfig,
    scen: ScenarioConfig,
    calib_map: Dict[int, WindowCalib],
    S0_ref: NDArray,
    policy: torch.nn.Module,
):
    """ì‹œë‚˜ë¦¬ì˜¤ I/II ì— ëŒ€í•´ ARL1 í‰ê°€ + CSV ì €ì¥."""
    lam2_list = [0.25, 0.50, 1.00, 2.00, 3.00, 5.00, 7.00, 9.00]
    lam_list = [math.sqrt(x) for x in lam2_list]

    # ì‹¤í–‰ í´ë” ì˜ˆ: outputs/run_20251113_153012/
    base_dir = os.path.dirname(cfg.S0_ref_path)  # ì´ë¯¸ runnerì—ì„œ ì„¸íŒ…í•¨

    for scenario_name in ["I", "II"]:
        print(f"\n[í‰ê°€] Scenario {scenario_name} (action_set={cfg.action_set})")

        # --- ARL0 í‰ê°€ (ì •ìƒ ìƒíƒœ) ---
        arl0_mean, arl0_std = evaluate_arl0(
            scen_cfg=scen,
            scenario=scenario_name,
            policy=policy,
            actions=list(cfg.action_set),
            calib_map=calib_map,
            S0_ref=S0_ref,
            R=cfg.R,
            seed=cfg.seed,
            rf_backend=cfg.rf_backend,
            n_estimators_eval=cfg.n_estimators_eval,
        )

        arl_means, arl_stds = evaluate_arl1(
            scen_cfg=scen,
            lam_list=lam_list,
            scenario=scenario_name,
            policy=policy,
            actions=list(cfg.action_set),
            calib_map=calib_map,
            S0_ref=S0_ref,
            R=cfg.R,
            seed=cfg.seed,
            rf_backend=cfg.rf_backend,
            n_estimators_eval=cfg.n_estimators_eval,
        )

        # ---- CSV ì €ì¥ ----
        csv_path = os.path.join(base_dir, f"arl_results_scenario_{scenario_name}.csv")
        print(f"[ì €ì¥] {csv_path}")

        with open(csv_path, "w", encoding="utf-8", newline="") as f:
            writer = csv.writer(f)
            # ğŸ‘‡ ì»¬ëŸ¼ 2ê°œ ì¶”ê°€
            writer.writerow(["lambda2", "lambda", "arl1_mean", "arl1_std", "arl0_mean", "arl0_std"])
            for lam2, lam, mean, std in zip(lam2_list, lam_list, arl_means, arl_stds):
                writer.writerow([lam2, lam, mean, std, arl0_mean, arl0_std])

        # ---- ì½˜ì†” ì¶œë ¥ ----
        print(f"  ARL0={arl0_mean:.2f} [{arl0_std:.2f}]")
        
        for lam2, lam, mean, std in zip(lam2_list, lam_list, arl_means, arl_stds):
            print(f"  Î»Â²={lam2:.2f} Î»={lam:.4f} ARL1={mean:.2f} [{std:.2f}]")


def main():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰: Phase I â†’ CL ë³´ì • â†’ RL í•™ìŠµ â†’ ARL1 í‰ê°€."""
    start_time = datetime.now()

    # 1) ì¸ì íŒŒì‹±
    parser = build_arg_parser()
    args = parser.parse_args()
    cfg = config_from_args(args)

    # ğŸ”½ğŸ”½ğŸ”½ ì—¬ê¸°ë¶€í„° ì¶”ê°€: ì¶œë ¥ í´ë” ì„¸íŒ… ğŸ”½ğŸ”½ğŸ”½
    # 1) ì‹¤í—˜ ì´ë¦„ ê²°ì •
    if cfg.exp_name is None or cfg.exp_name == "":
        exp_name = datetime.now().strftime("run_%Y%m%d_%H%M%S")
    else:
        exp_name = cfg.exp_name

    # 2) base_dir = outputs/run_...
    base_dir = os.path.join(cfg.outputs_dir, exp_name)
    os.makedirs(base_dir, exist_ok=True)
    print(f"[ì¶œë ¥] ê²°ê³¼ê°€ ì €ì¥ë  í´ë”: {base_dir}")

    # 3) S0_ref / calib_map / policy_out ì´ ë¹„ì–´ ìˆìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ ìë™ ì„¤ì •
    if not cfg.S0_ref_path:
        cfg.S0_ref_path = os.path.join(base_dir, "S0_ref.npy")
    if not cfg.calib_map_path:
        cfg.calib_map_path = os.path.join(base_dir, "calib_map.pkl")
    if not cfg.policy_out:
        cfg.policy_out = os.path.join(base_dir, "policy.pt")

    # (ì›í•˜ë©´ ARL ê²°ê³¼, ì„¤ì •ê°’ ì €ì¥ìš© ê¸°ë³¸ ê²½ë¡œë„ ë¯¸ë¦¬ ì¡ì•„ë‘˜ ìˆ˜ ìˆìŒ)
    # cfg.arl_results_path = os.path.join(base_dir, "arl_results.csv")
    # cfg.config_dump_path = os.path.join(base_dir, "config.txt")


    print(f"[ì‹œì‘] seed={cfg.seed}, device={cfg.device}, backend={cfg.rf_backend}")
    set_seed(cfg.seed)

    rng = check_random_state(cfg.seed)

    # 2) ì‹œë‚˜ë¦¬ì˜¤/ë°ì´í„° ì„¤ì • (ë…¼ë¬¸ ê¸°ë³¸ê°’)
    scen = ScenarioConfig(d=10, N0=1500, T=300, shift_time=100, sigma=1.0)
    
    # 3) Phase I ë°ì´í„° ì¤€ë¹„
    S0_ref = _prepare_phase1_data(cfg, scen, rng)

    # âœ… (ì„ íƒ) ë°±ì—”ë“œ ë²¤ì¹˜ë§ˆí¬ - ì •ìƒ ë²„ì „
    try:
        elapsed = run_backend_benchmark(
            S0_ref=S0_ref,
            d=scen.d,
            n_estimators=cfg.n_estimators_eval,
            seed=cfg.seed,
            backend=cfg.rf_backend,
        )
        print(f"[ë²¤ì¹˜ë§ˆí¬] backend='{cfg.rf_backend}' ê¸°ì¤€ 1íšŒ í†µê³„ ê³„ì‚° ì‹œê°„ â‰ˆ {elapsed:.3f} ì´ˆ")
    except Exception as e:
        print(f"[ë²¤ì¹˜ë§ˆí¬] ì‹¤íŒ¨ (ë¬´ì‹œí•´ë„ ë¨): {e}")

    # 4) CL ë³´ì •
    calib_map = _prepare_cl_calib(cfg, scen, S0_ref)

    # 5) RL ì •ì±… í•™ìŠµ
    policy = _train_policy(cfg, scen, calib_map, S0_ref)

    # 6) ARL1 í‰ê°€
    _evaluate(cfg, scen, calib_map, S0_ref, policy)

    elapsed = datetime.now() - start_time
    print(f"\n[ì™„ë£Œ] ì „ì²´ ì†Œìš” ì‹œê°„: {elapsed}")


if __name__ == "__main__":
    main()

```

### `scripts\run_ai_rtc.py`

```python
# scripts/run_ai_rtc.py

import os
import sys

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(CURRENT_DIR, ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

from runner import main   

if __name__ == "__main__":
    main()

```

### `utils.py`

```python
# ai_rtc/utils.py
import os
import math
import random
import numpy as np
import torch
import torch.nn as nn


def _np1d(y, dtype=np.int32):
    a = np.asarray(y, dtype=dtype).ravel()
    return np.ascontiguousarray(a)

def _np2d(X, dtype=np.float32):
    A = np.asarray(X, dtype=dtype)
    if A.ndim == 1:
        A = A.reshape(1, -1)
    return np.ascontiguousarray(A)

def set_seed(seed: int = 1234):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

def sqrt_int(x: float) -> int:
    return max(1, int(round(math.sqrt(x))))

def _unwrap_model(m: nn.Module) -> nn.Module:
    # torch.compile ë˜í•‘ í•´ì œ
    if hasattr(m, "_orig_mod"):
        m = m._orig_mod
    # DataParallel ë˜í•‘ í•´ì œ
    if isinstance(m, nn.DataParallel):
        m = m.module
    return m

```
